{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, datasets\n",
    " \n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    " \n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    " \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm # progress bar\n",
    "\n",
    "import skimage\n",
    "from skimage import img_as_ubyte, img_as_float32\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from torchsummary import summary\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Parameters \n",
    "#########################################\n",
    "\n",
    "training_image_paths = glob('Objects365/train/*.jpg')\n",
    "validation_image_paths = glob('Objects365/val/*.jpg')\n",
    "patch_dim = 96\n",
    "train_dataset_length = 40000\n",
    "validation_dataset_length = 2000\n",
    "gap = 48\n",
    "jitter = 7\n",
    "train_batch_size = 10000\n",
    "validation_batch_size = 24\n",
    "num_epochs = 1500\n",
    "learn_rate = 0.0005\n",
    "save_after_epochs = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Utilities \n",
    "#########################################\n",
    "\n",
    "def imshow(img,text=None,should_save=False):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()  \n",
    "\n",
    "def show_plot(iteration,loss,fname):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.savefig(fname)\n",
    "    plt.show()\n",
    "    \n",
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        for i, t in enumerate(tensor):\n",
    "            t.mul_(self.std[i%3]).add_(self.mean[i%3])\n",
    "        return tensor\n",
    "\n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################\n",
    "# This class generates patches for training\n",
    "#########################################\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "  def __init__(self, image_paths, patch_dim, length, gap, jitter, transform=None):\n",
    "    self.image_paths = image_paths\n",
    "    self.patch_dim = patch_dim\n",
    "    self.length = length\n",
    "    self.gap = gap\n",
    "    self.jitter = jitter\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "  \n",
    "  def prep_patch(self, image):\n",
    "    # print('prep_patch image.shape', image.shape)\n",
    "    # for some patches, randomly downsample to as little as 100 total pixels\n",
    "    if(random.random() < .33):\n",
    "      pil_patch = Image.fromarray(image)\n",
    "      original_size = pil_patch.size\n",
    "      randpix = int(math.sqrt(random.random() * (95 * 95 - 10 * 10) + 10 * 10))\n",
    "      pil_patch = pil_patch.resize((randpix, randpix)) \n",
    "      pil_patch = pil_patch.resize(original_size) \n",
    "      np.copyto(image, np.array(pil_patch))\n",
    "\n",
    "    # randomly drop all but one color channel\n",
    "    chan_to_keep = random.randint(0, 2)\n",
    "    for i in range(0, 3):\n",
    "      if i != chan_to_keep:\n",
    "        image[:,:,i] = np.random.randint(0, 255, (self.patch_dim, self.patch_dim), dtype=np.uint8)\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # [y, x, chan], dtype=uint8, top_left is (0,0)\n",
    "        \n",
    "    patch_loc_arr = [(-1, -1), (-1, 0), (-1, 1), (0, 1), (1, 1), (1, 0), (1, -1), (0, -1)]\n",
    "    \n",
    "    image_index = int(math.floor((len(self.image_paths) * random.random())))\n",
    "    \n",
    "\n",
    "\n",
    "    pil_image = Image.open(self.image_paths[image_index]).convert('RGB')\n",
    "\n",
    "    # Imagenet 150000 -> 180000 -> 450000\n",
    "    # 0.826 -> 2.479\n",
    "    # Objects365 300000 -> 370000 -> 925000\n",
    "\n",
    "    # original_size = pil_image.size\n",
    "    # randpix = int(math.sqrt(random.random() * (95 * 95 - 10 * 10) + 10 * 10))\n",
    "    # pil_image = pil_image.resize((randpix, randpix)) \n",
    "\n",
    "\n",
    "    image = np.array(pil_image)\n",
    "\n",
    "\n",
    "    # If image is too small, try another image\n",
    "    min_width = 3*patch_dim + 2*jitter + 2*gap\n",
    "    if (image.shape[0] - min_width) <= 0 or (image.shape[1] - min_width) <= 0:\n",
    "        # print(\"trying another image\")\n",
    "        return self.__getitem__(index)\n",
    "    \n",
    "    #print('__getitem__ image.shape', image.shape, self.margin, (image.shape[0] - self.margin*2), (image.shape[1] - self.margin*2))\n",
    "    \n",
    "    margin = math.ceil(patch_dim/2.0) + jitter\n",
    "\n",
    "    patch_direction_label = int(math.floor((8 * random.random())))\n",
    "    \n",
    "    patch_jitter_y = int(math.floor((self.jitter * 2 * random.random()))) - self.jitter\n",
    "    patch_jitter_x = int(math.floor((self.jitter * 2 * random.random()))) - self.jitter\n",
    "        \n",
    "    while True:\n",
    "        \n",
    "        uniform_patch_y_coord = int(math.floor((image.shape[0] - margin*2) * random.random())) + margin - int(round(self.patch_dim/2.0))\n",
    "        uniform_patch_x_coord = int(math.floor((image.shape[1] - margin*2) * random.random())) + margin - int(round(self.patch_dim/2.0))\n",
    "\n",
    "        random_patch_y_coord = uniform_patch_y_coord + patch_loc_arr[patch_direction_label][0] * (self.patch_dim + self.gap) + patch_jitter_y\n",
    "        random_patch_x_coord = uniform_patch_x_coord + patch_loc_arr[patch_direction_label][1] * (self.patch_dim + self.gap) + patch_jitter_x\n",
    "\n",
    "        if random_patch_y_coord >= 0 and random_patch_x_coord >= 0 and random_patch_y_coord < (image.shape[0] - patch_dim) and random_patch_x_coord < (image.shape[1] - patch_dim):\n",
    "            #print(\"found\")\n",
    "            break\n",
    "            \n",
    "        #print(\"not found\")\n",
    "\n",
    "    uniform_patch = image[uniform_patch_y_coord:uniform_patch_y_coord+self.patch_dim, uniform_patch_x_coord:uniform_patch_x_coord+self.patch_dim]        \n",
    "    random_patch = image[random_patch_y_coord:random_patch_y_coord+self.patch_dim, random_patch_x_coord:random_patch_x_coord+self.patch_dim]\n",
    "\n",
    "    # print('__getitem__ patch coords', uniform_patch_y_coord, uniform_patch_y_coord, random_patch_x_coord, random_patch_x_coord)\n",
    "    \n",
    "#     if(index % 1000 == 0):\n",
    "#         print('__getitem__', index, patch_direction_label, self.image_paths[image_index])\n",
    "        \n",
    "#     self.prep_patch(uniform_patch)\n",
    "#     self.prep_patch(random_patch)\n",
    "\n",
    "    patch_direction_label = np.array(patch_direction_label).astype(np.int64)\n",
    "        \n",
    "    if self.transform:\n",
    "      uniform_patch = self.transform(uniform_patch)\n",
    "      random_patch = self.transform(random_patch)\n",
    "\n",
    "\n",
    "\n",
    "    return uniform_patch, random_patch, patch_direction_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Creating Train/Validation dataset and dataloader\n",
    "##################################################\n",
    "\n",
    "traindataset = MyDataset(training_image_paths, patch_dim, train_dataset_length, gap, jitter,\n",
    "                         transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(traindataset, \n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "valdataset = MyDataset(validation_image_paths, patch_dim, validation_dataset_length, gap, jitter,\n",
    "                         transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valdataset,\n",
    "                                        batch_size=validation_batch_size,\n",
    "                                        shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 1353, 2: 1325, 7: 1243, 5: 1225, 6: 1220, 0: 1218, 1: 1217, 3: 1199})\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Visualizing validation dataset\n",
    "#############################\n",
    "\n",
    "example_batch_val = next(iter(trainloader))\n",
    "# concatenated = torch.cat((unorm(example_batch_val[0]),unorm(example_batch_val[1])),0)\n",
    "# imshow(torchvision.utils.make_grid(concatenated))\n",
    "# print(f'Labels: {example_batch_val[2].numpy()}')\n",
    "print(collections.Counter(example_batch_val[2].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################################\n",
    "# Model for learning patch position\n",
    "##################################################\n",
    "\n",
    "class AlexNetwork(nn.Module):\n",
    "  def __init__(self,aux_logits = False):\n",
    "\n",
    "      super(AlexNetwork, self).__init__()\n",
    "\n",
    "      self.cnn = nn.Sequential(\n",
    "        nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(128), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(128), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "      )\n",
    "    \n",
    "      self.fc6 = nn.Sequential(\n",
    "        nn.Linear(512 * 3 * 3, 4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(),\n",
    "      )\n",
    "\n",
    "      self.fc = nn.Sequential(\n",
    "        nn.Linear(2*4096, 4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(4096, 8),\n",
    "      )\n",
    "    \n",
    "    \n",
    "#       self.cnn = nn.Sequential(\n",
    "#         nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=5),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#         nn.LocalResponseNorm(5),\n",
    "        \n",
    "#         nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#         nn.LocalResponseNorm(5),\n",
    "        \n",
    "#         nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(384),\n",
    "#         nn.ReLU(inplace=True),\n",
    "        \n",
    "#         nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(384),\n",
    "#         nn.ReLU(inplace=True),\n",
    "        \n",
    "#         nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.ReLU(inplace=True),\n",
    "        \n",
    "#         nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#       )\n",
    "    \n",
    "#       self.cnn = nn.Sequential(\n",
    "#         nn.Conv2d(3, 96, kernel_size=11, stride=4),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#         nn.LocalResponseNorm(96),\n",
    "        \n",
    "#         nn.Conv2d(96, 384, kernel_size=5, stride = 2,padding = 2),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#         nn.LocalResponseNorm(384),\n",
    "        \n",
    "#         nn.Conv2d(384, 384, kernel_size=3, stride=1,padding = 1),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.BatchNorm2d(384),\n",
    "        \n",
    "#         nn.Conv2d(384, 384, kernel_size=3, stride=1,padding = 1),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.BatchNorm2d(384),\n",
    "        \n",
    "#         nn.Conv2d(384, 256, kernel_size=3, stride=1,padding = 1),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.MaxPool2d(kernel_size=3, stride=2,padding = 1),\n",
    "#       )\n",
    "\n",
    "#       self.fc6 = nn.Sequential(\n",
    "#         nn.Linear((512 * 3 * 3),4096),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.BatchNorm1d(4096),\n",
    "#       )\n",
    "\n",
    "#       self.fc = nn.Sequential(\n",
    "#         nn.Linear(2*4096,4096),\n",
    "#         nn.ReLU(inplace=True),\n",
    "\n",
    "#         nn.Linear(4096, 4096),\n",
    "#         nn.ReLU(inplace=True),\n",
    "\n",
    "#         nn.Linear(4096, 8)\n",
    "#       )\n",
    "\n",
    "  def forward_once(self, x):\n",
    "    output= self.cnn(x)\n",
    "    #print('a', output.size())\n",
    "    output = output.view(output.size()[0], -1)\n",
    "    #print('b', output.size())\n",
    "    output = self.fc6(output)\n",
    "    return output\n",
    "\n",
    "  def forward(self, uniform_patch, random_patch):\n",
    "    output_fc6_uniform = self.forward_once(uniform_patch)\n",
    "    output_fc6_random = self.forward_once(random_patch)\n",
    "    output = torch.cat((output_fc6_uniform,output_fc6_random), 1)\n",
    "    output = self.fc(output)\n",
    "    return output, output_fc6_uniform, output_fc6_random\n",
    "\n",
    "model = AlexNetwork().to(device)\n",
    "summary(model, [(3, 96, 96), (3, 96, 96)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Initialized Optimizer, criterion, scheduler\n",
    "#############################################\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                           mode='min',\n",
    "                                           patience=5,\n",
    "                                           factor=0.3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################\n",
    "# Training/Validation Engine\n",
    "############################\n",
    "\n",
    "global_trn_loss = []\n",
    "global_val_loss = []\n",
    "# previous_val_loss = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_running_loss = []\n",
    "    val_running_loss = []\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for idx, data in tqdm(enumerate(trainloader), total=int(len(traindataset)/train_batch_size)):\n",
    "        uniform_patch, random_patch, random_patch_label = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "        # print(uniform_patch.size(), random_patch.size())\n",
    "        optimizer.zero_grad()\n",
    "        output, output_fc6_uniform, output_fc6_random = model(uniform_patch, random_patch)\n",
    "        loss = criterion(output, random_patch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_running_loss.append(loss.item())\n",
    "    else:\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        for idx, data in tqdm(enumerate(valloader), total=int(len(valdataset)/validation_batch_size)):\n",
    "          uniform_patch, random_patch, random_patch_label = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "          output, output_fc6_uniform, output_fc6_random = model(uniform_patch, random_patch)\n",
    "          loss = criterion(output, random_patch_label)\n",
    "          val_running_loss.append(loss.item())\n",
    "        \n",
    "          _, predicted = torch.max(output.data, 1)\n",
    "          total += random_patch_label.size(0)\n",
    "          correct += (predicted == random_patch_label).sum()\n",
    "        print('Val Progress --- total:{}, correct:{}'.format(total, correct.item()))\n",
    "        print('Val Accuracy of the network on the test images: {}%'.format(100 * correct.item() / total))\n",
    "\n",
    "    global_trn_loss.append(sum(train_running_loss) / len(train_running_loss))\n",
    "    global_val_loss.append(sum(val_running_loss) / len(val_running_loss))\n",
    "\n",
    "    scheduler.step(global_val_loss[-1])\n",
    "\n",
    "    print('Epoch [{}/{}], TRNLoss:{:.4f}, VALLoss:{:.4f}, Time:{:.2f}'.format(\n",
    "        epoch + 1, num_epochs, global_trn_loss[-1], global_val_loss[-1],\n",
    "        (time.time() - start_time) / 60))\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "      MODEL_SAVE_PATH = f'model_{train_batch_size}_{num_epochs}_{learn_rate}_{patch_dim}_{gap}.pt'\n",
    "      torch.save(\n",
    "        {\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            'global_trnloss': global_trn_loss,\n",
    "            'global_valloss': global_val_loss\n",
    "        }, MODEL_SAVE_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
