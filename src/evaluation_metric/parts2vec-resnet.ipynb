{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import ipyplot\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import compress\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import ipyplot\n",
    "import gensim\n",
    "from ast import literal_eval\n",
    "\n",
    "from ShufflePatchModel16 import ShufflePatchFeatureExtractor\n",
    "from VggFeatureExtractor import VggFeatureExtractor\n",
    "from MoCoFeatureExtractor import MoCoFeatureExtractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and Utility methods for extracting patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 96\n",
    "stride = 24\n",
    "kp_margin = 16 # keypoint detector has a margin around image where it can not find keypoints\n",
    "n_clusters = 100\n",
    "\n",
    "max_files = 1000\n",
    "\n",
    "walk_length = 10\n",
    "walks_per_image = 100\n",
    "word_format_string = '{:02d}'\n",
    "\n",
    "feature_lenth = 2048\n",
    "\n",
    "image_scales = [1/3]\n",
    "\n",
    "cnn = MoCoFeatureExtractor()\n",
    "#cnn = VggFeatureExtractor()\n",
    "#cnn = ShufflePatchFeatureExtractor(\"/Users/racoon/Desktop/rotation_jigsaw_migrated_0710_0.0001_1.9522_43.62.pt\")\n",
    "\n",
    "def extract_windows(frame, pos, window_size):\n",
    "    windows = np.empty((len(pos), window_size, window_size, 3), dtype=np.uint8)\n",
    "\n",
    "    for i in range(len(pos)):\n",
    "        windows[i] = extract_window(frame, pos[i], window_size)\n",
    "\n",
    "    return windows\n",
    "\n",
    "\n",
    "def extract_window(frame, pos, window_size):\n",
    "    half_w = window_size/2.0\n",
    "\n",
    "    top_left = [int(round(pos[0]-half_w)), int(round(pos[1]-half_w))]\n",
    "    bottom_right = [top_left[0]+window_size, top_left[1]+window_size]\n",
    "\n",
    "    return frame[top_left[0]:bottom_right[0], top_left[1]:bottom_right[1]]\n",
    "\n",
    "\n",
    "\n",
    "def get_rad_grid(grid_pos, rad, grid_shape):\n",
    "\n",
    "    top_left = (grid_pos[0]-rad, grid_pos[1]-rad)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for i in range(2*rad+1):\n",
    "        p = (top_left[0]+i, top_left[1])\n",
    "        if p[0] >= 0 and p[1] >= 0 and p[0] < grid_shape[0] and p[1] < grid_shape[1]:\n",
    "            res.append(p)\n",
    " \n",
    "    for i in range(2*rad+1):\n",
    "        p = (top_left[0]+i, top_left[1]+(2*rad))\n",
    "        if p[0] >= 0 and p[1] >= 0 and p[0] < grid_shape[0] and p[1] < grid_shape[1]:\n",
    "            res.append(p)\n",
    "\n",
    "    for i in range(2*rad-1):\n",
    "        p = (top_left[0], top_left[1]+(i+1))\n",
    "        if p[0] >= 0 and p[1] >= 0 and p[0] < grid_shape[0] and p[1] < grid_shape[1]:\n",
    "            res.append(p)\n",
    "\n",
    "    for i in range(2*rad-1):\n",
    "        p = (top_left[0]+(2*rad), top_left[1]+(i+1))\n",
    "        if p[0] >= 0 and p[1] >= 0 and p[0] < grid_shape[0] and p[1] < grid_shape[1]:\n",
    "            res.append(p)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def next_pos(salient_grid_positions, grid_shape, current_position):\n",
    "    \n",
    "    if current_position is not None:\n",
    "\n",
    "        rad_grid = get_rad_grid(current_position, 1, grid_shape)\n",
    "\n",
    "        # print('rad_grid', current_position, rad_grid)\n",
    "        \n",
    "        if len(rad_grid) == 0:\n",
    "            print(\"frame empty?\")\n",
    "            \n",
    "        else:\n",
    "            random.shuffle(rad_grid)\n",
    "            for loc in rad_grid:\n",
    "                if loc in salient_grid_positions:\n",
    "                    return loc\n",
    "    \n",
    "    return random.sample(salient_grid_positions,1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample and cluster patches\n",
    "This simply uses a fixed grid system. Future patch sampling methods could incorporate an intrest point detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3333333333333333 dataset_100/train/car/46938b4c628ce00e.jpg\n",
      "feats.shape (24, 2048, 1, 1) (24, 128, 128, 3)\n",
      "1 0.3333333333333333 dataset_100/train/car/8c4b9d096f6423ed.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "2 0.3333333333333333 dataset_100/train/car/fc418b3caef440aa.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "3 0.3333333333333333 dataset_100/train/car/eac45380074ba8c8.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "4 0.3333333333333333 dataset_100/train/car/ea9a6d46a1279f85.jpg\n",
      "feats.shape (16, 2048, 1, 1) (16, 128, 128, 3)\n",
      "5 0.3333333333333333 dataset_100/train/car/9893ae3d876f9c1c.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "6 0.3333333333333333 dataset_100/train/car/f6b73bb2536fdcb7.jpg\n",
      "feats.shape (64, 2048, 1, 1) (64, 128, 128, 3)\n",
      "7 0.3333333333333333 dataset_100/train/car/c1ae01ffc0c505f4.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "8 0.3333333333333333 dataset_100/train/car/0a41cda5f44baaf6.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "9 0.3333333333333333 dataset_100/train/car/0c9f9b713f229fba.jpg\n",
      "feats.shape (24, 2048, 1, 1) (24, 128, 128, 3)\n",
      "10 0.3333333333333333 dataset_100/train/car/3fe04f7604431846.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "11 0.3333333333333333 dataset_100/train/car/d14658b78cec2cf7.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "12 0.3333333333333333 dataset_100/train/car/cef82902c2f6cac2.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "13 0.3333333333333333 dataset_100/train/car/87a618d5e8e769f4.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "14 0.3333333333333333 dataset_100/train/car/240d6d636ae47e4c.jpg\n",
      "feats.shape (24, 2048, 1, 1) (24, 128, 128, 3)\n",
      "15 0.3333333333333333 dataset_100/train/car/0104cba3ff87376c.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "16 0.3333333333333333 dataset_100/train/car/6b27c58d912aef06.jpg\n",
      "feats.shape (24, 2048, 1, 1) (24, 128, 128, 3)\n",
      "17 0.3333333333333333 dataset_100/train/car/d9693518440b0b06.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "18 0.3333333333333333 dataset_100/train/car/068921685a717645.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "19 0.3333333333333333 dataset_100/train/car/66fb599a59a53707.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "20 0.3333333333333333 dataset_100/train/car/e79c15a7a0768fc7.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "21 0.3333333333333333 dataset_100/train/car/339b043b9f762f10.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "22 0.3333333333333333 dataset_100/train/car/f3c07279aed5f8d9.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "23 0.3333333333333333 dataset_100/train/car/b7ac79133f230248.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "24 0.3333333333333333 dataset_100/train/car/8e01b9d1471238ff.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "25 0.3333333333333333 dataset_100/train/car/18601389796b9296.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "26 0.3333333333333333 dataset_100/train/car/3c613b1aa9fa1bbf.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "27 0.3333333333333333 dataset_100/train/car/c6f0bd65f2c04cc8.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "28 0.3333333333333333 dataset_100/train/car/538dcb5018297a56.jpg\n",
      "feats.shape (16, 2048, 1, 1) (16, 128, 128, 3)\n",
      "29 0.3333333333333333 dataset_100/train/car/c8698f0073819905.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "30 0.3333333333333333 dataset_100/train/car/4fc6cb9df6ea675f.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "31 0.3333333333333333 dataset_100/train/car/5206ff75a1e480b0.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "32 0.3333333333333333 dataset_100/train/car/6cb37347c1ff659f.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "33 0.3333333333333333 dataset_100/train/car/7306f88e950a43b8.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "34 0.3333333333333333 dataset_100/train/car/79d158aa4408e5fe.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "35 0.3333333333333333 dataset_100/train/car/8ea87a001d8fed5c.jpg\n",
      "feats.shape (16, 2048, 1, 1) (16, 128, 128, 3)\n",
      "36 0.3333333333333333 dataset_100/train/car/6ec05587fb50f81d.jpg\n",
      "feats.shape (24, 2048, 1, 1) (24, 128, 128, 3)\n",
      "37 0.3333333333333333 dataset_100/train/car/d69642784c6741ec.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "38 0.3333333333333333 dataset_100/train/car/50bc64a88e14f9fb.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "39 0.3333333333333333 dataset_100/train/car/230960886121a41a.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "40 0.3333333333333333 dataset_100/train/car/50784a0961216133.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "41 0.3333333333333333 dataset_100/train/car/dafb540b5ee8488a.jpg\n",
      "feats.shape (24, 2048, 1, 1) (24, 128, 128, 3)\n",
      "42 0.3333333333333333 dataset_100/train/car/88e5a3fe3fb0b71e.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "43 0.3333333333333333 dataset_100/train/car/8b08690c2f1e57ac.jpg\n",
      "feats.shape (16, 2048, 1, 1) (16, 128, 128, 3)\n",
      "44 0.3333333333333333 dataset_100/train/car/eecd66089649f982.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "45 0.3333333333333333 dataset_100/train/car/02c0f07948ac1eb9.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "46 0.3333333333333333 dataset_100/train/car/0e28560eebe0f1f4.jpg\n",
      "feats.shape (8, 2048, 1, 1) (8, 128, 128, 3)\n",
      "47 0.3333333333333333 dataset_100/train/car/a37ef652b5a253ba.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "48 0.3333333333333333 dataset_100/train/car/7e1ae78fe2c0882f.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "49 0.3333333333333333 dataset_100/train/car/afe1daabdf696287.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "50 0.3333333333333333 dataset_100/train/car/988abe381c6daf10.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "51 0.3333333333333333 dataset_100/train/car/d5c700f6f7d5d15f.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "52 0.3333333333333333 dataset_100/train/car/456439ac45f0f784.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "53 0.3333333333333333 dataset_100/train/car/2d1397e3f603b83b.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "54 0.3333333333333333 dataset_100/train/car/c4b4aad15b0d20b6.jpg\n",
      "feats.shape (48, 2048, 1, 1) (48, 128, 128, 3)\n",
      "55 0.3333333333333333 dataset_100/train/car/23698d4b7ec48819.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "56 0.3333333333333333 dataset_100/train/car/5e9ef1ccc144e4d5.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "57 0.3333333333333333 dataset_100/train/car/72d9aa6212accc52.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "58 0.3333333333333333 dataset_100/train/car/dda6a3de709a89cc.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "59 0.3333333333333333 dataset_100/train/car/1f34454c4c073e1b.jpg\n",
      "feats.shape (30, 2048, 1, 1) (30, 128, 128, 3)\n",
      "60 0.3333333333333333 dataset_100/train/car/e1b101465df68788.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "61 0.3333333333333333 dataset_100/train/car/a87aabe7ced084da.jpg\n",
      "feats.shape (56, 2048, 1, 1) (56, 128, 128, 3)\n",
      "62 0.3333333333333333 dataset_100/train/car/fdd963edc28bf163.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "63 0.3333333333333333 dataset_100/train/car/10deaafff8740957.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "64 0.3333333333333333 dataset_100/train/car/40ab29a060cb9eb4.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "65 0.3333333333333333 dataset_100/train/car/8aaa6d4955ffe5b7.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "66 0.3333333333333333 dataset_100/train/car/1ed277062bc9b8b8.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "67 0.3333333333333333 dataset_100/train/car/33922016c7c98c3b.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "68 0.3333333333333333 dataset_100/train/car/be51f7840abc3b89.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "69 0.3333333333333333 dataset_100/train/car/dd372c93febdfb3b.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "70 0.3333333333333333 dataset_100/train/car/0b1359c23700cb4c.jpg\n",
      "feats.shape (16, 2048, 1, 1) (16, 128, 128, 3)\n",
      "71 0.3333333333333333 dataset_100/train/car/f24929aff8c7b260.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "72 0.3333333333333333 dataset_100/train/car/d29650a6404a2fff.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "73 0.3333333333333333 dataset_100/train/car/9ff761715da8db4a.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "74 0.3333333333333333 dataset_100/train/car/60f2e6435743ea37.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "75 0.3333333333333333 dataset_100/train/car/dcd02dd2a9cdc670.jpg\n",
      "feats.shape (16, 2048, 1, 1) (16, 128, 128, 3)\n",
      "76 0.3333333333333333 dataset_100/train/car/0a7be0b883a12966.jpg\n",
      "feats.shape (16, 2048, 1, 1) (16, 128, 128, 3)\n",
      "77 0.3333333333333333 dataset_100/train/car/c242644c195aef4b.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "78 0.3333333333333333 dataset_100/train/car/126c44183c41ab28.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "79 0.3333333333333333 dataset_100/train/car/d3729058c6a4186e.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "80 0.3333333333333333 dataset_100/train/car/836f610f7271d595.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "81 0.3333333333333333 dataset_100/train/car/45c5a5823458b1e0.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "82 0.3333333333333333 dataset_100/train/car/5ff817e7b606a605.jpg\n",
      "feats.shape (32, 2048, 1, 1) (32, 128, 128, 3)\n",
      "83 0.3333333333333333 dataset_100/train/car/9db4b3227af3862e.jpg\n",
      "feats.shape (40, 2048, 1, 1) (40, 128, 128, 3)\n",
      "84 0.3333333333333333 dataset_100/train/car/500e127b95028a6d.jpg\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "non-empty 3D or 4D input tensor expected but got ndim: 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-130f44bc24e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevalRGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feats.shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_lenth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/sc_patch/src/evaluation_metric/MoCoFeatureExtractor.py\u001b[0m in \u001b[0;36mevalRGB\u001b[0;34m(self, patches)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    158\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                             self.return_indices)\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     return torch.max_pool2d(\n\u001b[0m\u001b[1;32m    576\u001b[0m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: non-empty 3D or 4D input tensor expected but got ndim: 4"
     ]
    }
   ],
   "source": [
    "image_files = glob(\"dataset_100/train/*/*.jpg\")[:max_files]\n",
    "\n",
    "X = []\n",
    "\n",
    "for idx, image_file in enumerate(image_files):\n",
    "    print(idx, image_scale, image_file)\n",
    "    pil_image = Image.open(image_file).convert('RGB')\n",
    "    pil_image = pil_image.resize((int(round(pil_image.size[0] * image_scale)), int(round(pil_image.size[1] * image_scale))))\n",
    "    image = np.array(pil_image)\n",
    "\n",
    "    margin = max(window_size, kp_margin*2)\n",
    "    grid_shape = (math.floor((image.shape[0] - margin) / stride), math.floor((image.shape[1] - margin) / stride))\n",
    "    offsets = (round((image.shape[0] - grid_shape[0] * stride)/2), round((image.shape[1] - grid_shape[1] * stride)/2))\n",
    "\n",
    "    points = [(offsets[0]+y*stride+stride/2,offsets[1]+x*stride+stride/2) for y in range(grid_shape[0]) for x in range(grid_shape[1])]\n",
    "\n",
    "    patches = extract_windows(image, points, window_size)\n",
    "\n",
    "    #P.extend(list(patches))\n",
    "\n",
    "    windows = patches.astype(np.float64)\n",
    "\n",
    "    feats = cnn.evalRGB(windows)\n",
    "    print('feats.shape', feats.shape, windows.shape)\n",
    "    feats = feats.reshape((windows.shape[0], feature_lenth))\n",
    "    X.extend(list(feats))\n",
    "\n",
    "\n",
    "print(\"Clustering with KMeans\")\n",
    "clusters = KMeans(n_clusters=n_clusters, verbose=False)\n",
    "clusters.fit(np.array(X, dtype=np.float32))\n",
    "\n",
    "cluster_count = len(np.unique(clusters.labels_))\n",
    "print(\"done\")\n",
    "\n",
    "pickle.dump(clusters, open(\"clusters.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sequence dataset with random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salient_grid_locations(image, stride, grid_shape, offsets, orb, mask = None):\n",
    "    \n",
    "    if mask is not None:\n",
    "        object_grid_locations = set()\n",
    "\n",
    "        for y in range(grid_shape[0]):\n",
    "            for x in range(grid_shape[1]):\n",
    "                p = (offsets[0] + y * stride + 0.5 * stride, offsets[1] + x * stride + 0.5 * stride)\n",
    "                w = extract_window(mask, p, stride)\n",
    "\n",
    "                # print(np.sum(w))\n",
    "                if np.sum(w) >= stride * stride * 0.3:\n",
    "                    object_grid_locations.add((y, x))\n",
    "                \n",
    "                \n",
    "    kp = orb.detect(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), None)\n",
    "\n",
    "    grid = set()\n",
    "\n",
    "    for k in kp:\n",
    "        p = (k.pt[1], k.pt[0]) \n",
    "        g = (int(math.floor((p[0]-offsets[0])/stride)), int(math.floor((p[1]-offsets[1])/stride)))\n",
    "        \n",
    "        if g[0] < 0 or g[0] >= grid_shape[0] or g[1] < 0 or g[1] >= grid_shape[1]:\n",
    "            continue\n",
    "            \n",
    "        if mask is not None: \n",
    "            if g in object_grid_locations:\n",
    "                grid.add(g)\n",
    "        else:\n",
    "             grid.add(g)\n",
    "            \n",
    "    \n",
    "    return grid\n",
    "\n",
    "\n",
    "def generate_image_sequences(image_file, image_scale, clusters, feature_extractor, orb, mask_file = None, seq_count=walks_per_image):\n",
    "    # print(\"generate_image_sequences\", image_file)\n",
    "\n",
    "    pil_image = Image.open(image_file).convert('RGB')\n",
    "    pil_image = pil_image.resize((int(round(pil_image.size[0] * image_scale)), int(round(pil_image.size[1] * image_scale))))\n",
    "    image = np.array(pil_image)\n",
    "\n",
    "    mask = None\n",
    "    \n",
    "    if mask_file is not None:\n",
    "        pil_mask = Image.open(mask_file).convert('1')\n",
    "        pil_mask = pil_mask.resize((int(round(pil_mask.size[0] * image_scale)), int(round(pil_mask.size[1] * image_scale))))\n",
    "        mask = np.array(pil_mask)\n",
    "        \n",
    "    \n",
    "    margin = max(window_size, kp_margin*2)\n",
    "    grid_shape = (math.floor((image.shape[0] - margin) / stride), math.floor((image.shape[1] - margin) / stride))\n",
    "    offsets = (round((image.shape[0] - grid_shape[0] * stride)/2), round((image.shape[1] - grid_shape[1] * stride)/2))\n",
    "\n",
    "    grid_locations_set = salient_grid_locations(image, stride, grid_shape, offsets, orb, mask)\n",
    "    grid_locations_list = list(grid_locations_set)\n",
    "    \n",
    "    # print(grid_shape, grid_locations_list)\n",
    "    points = [(y*stride + stride/2 + offsets[0], x*stride + stride/2 + offsets[1]) for (y,x) in grid_locations_list]\n",
    "    \n",
    "    # print('salient grid locations', grid_shape, len(grid_locations_list), 'of', grid_shape[0] * grid_shape[1])\n",
    "        \n",
    "    patches = extract_windows(image, points, window_size)\n",
    "    windows = patches.astype(np.float64)\n",
    "\n",
    "    feats = cnn.evalRGB(windows)\n",
    "    feats = feats.reshape((windows.shape[0], feature_lenth))\n",
    "\n",
    "    grid_cluster_ids = clusters.predict(feats)\n",
    "\n",
    "    grid_location_to_cluster_id = dict([(grid_locations_list[i], grid_cluster_ids[i]) for i in range(len(grid_locations_list))])\n",
    "    grid_location_to_patches = dict([(grid_locations_list[i], patches[i]) for i in range(len(grid_locations_list))])\n",
    "        \n",
    "    cluster_seqs = []\n",
    "    point_seqs = []\n",
    "    \n",
    "    for i in range(seq_count):\n",
    "        cluster_seq = []\n",
    "        point_seq = []\n",
    "        #patch_seq = []\n",
    "        \n",
    "        pos = None\n",
    "        \n",
    "        for t in range(walk_length):\n",
    "            pos = next_pos(grid_locations_set, grid_shape, pos)\n",
    "            #patch_seq.append(grid_location_to_patches[pos])\n",
    "            cluster_seq.append(grid_location_to_cluster_id[pos])\n",
    "            point_seq.append((pos[0]*stride + stride/2 + offsets[0], pos[1]*stride + stride/2 + offsets[1]))\n",
    "            \n",
    "        cluster_seqs.append([word_format_string.format(w) for w in cluster_seq])\n",
    "        point_seqs.append(point_seq)\n",
    "        #print(seqs[-1])\n",
    "        \n",
    "        #ipyplot.plot_images(patch_seq, point_seq)\n",
    "        \n",
    "    return cluster_seqs, point_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = glob(\"dataset_100/train/*/*.jpg\")[:max_files]\n",
    "image_files.sort()\n",
    "\n",
    "clusters = pickle.load(open(\"clusters.pkl\", \"rb\"))\n",
    "orb = cv2.ORB_create(nfeatures=100000, fastThreshold=7)\n",
    "\n",
    "\n",
    "cluster_seqs = []\n",
    "point_seqs = []\n",
    "image_file_colummn = []\n",
    "image_scale_colummn = []\n",
    "\n",
    "for image_scale in image_scales:\n",
    "    for idx, image_file in enumerate(image_files):\n",
    "        print(idx, image_file)\n",
    "        c_seqs, p_seqs = generate_image_sequences(image_file, image_scale, clusters, cnn, orb)\n",
    "        cluster_seqs.extend(c_seqs)\n",
    "        point_seqs.extend(p_seqs)\n",
    "        image_file_colummn.extend([image_file] * walks_per_image)\n",
    "        image_scale_colummn.extend([image_scale] * walks_per_image)\n",
    "    \n",
    "    \n",
    "data_frame = pd.DataFrame({'words':cluster_seqs, 'file':image_file_colummn, 'points': point_seqs, 'scale': image_scale_colummn})\n",
    "data_frame.to_csv('sequences.csv')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['47', '47', '50', '14', '50', '47', '50', '47', '47', '47'], tags=[0]), TaggedDocument(words=['47', '47', '50', '50', '50', '50', '50', '50', '50', '14'], tags=[1])]\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "epoch 20\n",
      "epoch 21\n",
      "epoch 22\n",
      "epoch 23\n",
      "epoch 24\n",
      "epoch 25\n",
      "epoch 26\n",
      "epoch 27\n",
      "epoch 28\n",
      "epoch 29\n",
      "epoch 30\n",
      "epoch 31\n",
      "epoch 32\n",
      "epoch 33\n",
      "epoch 34\n",
      "epoch 35\n",
      "epoch 36\n",
      "epoch 37\n",
      "epoch 38\n",
      "epoch 39\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "class callback(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print('epoch {}'.format(self.epoch))\n",
    "        self.epoch += 1\n",
    "              \n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    data_frame = pd.read_csv('sequences.csv',converters={\"words\": literal_eval, \"points\": literal_eval})\n",
    "    \n",
    "    for index, row in data_frame.iterrows():\n",
    "        if tokens_only:\n",
    "            yield row['words']\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(row['words'], [index])\n",
    "\n",
    "train_corpus = list(read_corpus('sequences.csv'))\n",
    "print(train_corpus[:2])\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, epochs=40)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs, callbacks=[callback()])\n",
    "\n",
    "model.save('doc2vec.model')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset_100/test/airplane/35b11a04c24db20c.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/airplane/4ad0f079b979be5d.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/airplane/839ce813ca97084c.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/airplane/93b5bf58149adefd.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/airplane/9dc879c35a26d2d3.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/airplane/a48f1d15812036fa.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/airplane/b6ac22d7db1769ee.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/airplane/d5422871fd63b8b8.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/airplane/e95bc413d4b748ba.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/airplane/fbe835c5944f93e5.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/car/455c29cd8db5b225.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/car/56d1d8aca15ae219.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/car/89297009b1d18663.jpg\n",
      "test dataset_100/test/car/9bac3f90244fef7e.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/car/a051a80f600fd919.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/car/b30bc1fe86057942.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/car/cc0c6a5753fbe006.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/car/cdbfe2973b6fdaf2.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/car/d12414ad4d3e845e.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/car/d16cb785f98e3d1c.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/horse/167af8f2bde5db07.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/horse/18e5dbb67158438c.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/horse/405bd80086a4588d.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/horse/406512ee7efc2b43.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/horse/51ae02f6c585c444.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/horse/6b243472d75334b0.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/horse/865610e59aaa728f.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/horse/9b88ca5d575fc632.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/horse/b171e2886317f12d.jpg\n",
      "len(similar) 10\n",
      "test dataset_100/test/horse/f3cb2620040852c4.jpg\n",
      "len(similar) 10\n",
      "score 0.9844137931034482\n"
     ]
    }
   ],
   "source": [
    "clusters = pickle.load(open(\"clusters.pkl\", \"rb\"))\n",
    "orb = cv2.ORB_create(nfeatures=100000, fastThreshold=7)\n",
    "model = gensim.models.doc2vec.Doc2Vec.load('doc2vec.model')\n",
    "\n",
    "data_frame = pd.read_csv('sequences.csv',converters={\"words\": literal_eval, \"points\": literal_eval})\n",
    "\n",
    "test_image_files = glob(\"dataset_100/test/*/*.jpg\")\n",
    "test_mask_files = glob(\"dataset_100/test/*/*.mask.png\")\n",
    "\n",
    "test_image_files.sort()\n",
    "test_mask_files.sort()\n",
    "\n",
    "\n",
    "for image_scale in image_scales:\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(len(test_image_files)):\n",
    "\n",
    "        try:\n",
    "            similar_files_set = set()\n",
    "            \n",
    "            image_file = test_image_files[i]\n",
    "            mask_file = test_mask_files[i]\n",
    "\n",
    "            print(\"test\", image_file)\n",
    "\n",
    "            c_seqs, p_seqs = generate_image_sequences(image_file, image_scale, clusters, cnn, orb, mask_file = mask_file, seq_count=100)\n",
    "            #print('seqs', seqs)\n",
    "\n",
    "            vectors = [[model.infer_vector(s)] for s in c_seqs]\n",
    "            #print('vectors', vectors)\n",
    "\n",
    "            for v in vectors:\n",
    "                similar = model.docvecs.most_similar(v, topn=10)\n",
    "                #print('similar', similar)\n",
    "\n",
    "                for s in similar:\n",
    "                    f = data_frame.loc[s[0],'file']\n",
    "                    if '/airplane/' in image_file and '/airplane/' in f:\n",
    "                        similar_files_set.add(f)\n",
    "                        correct += 1\n",
    "                    elif '/car/' in image_file and '/car/' in f:\n",
    "                        similar_files_set.add(f)\n",
    "                        correct += 1\n",
    "                    elif '/horse/' in image_file and '/horse/' in f:\n",
    "                        similar_files_set.add(f)\n",
    "                        correct += 1\n",
    "\n",
    "                    #print(\"similar to\", f)\n",
    "\n",
    "                    total += 1\n",
    "                    \n",
    "            print('len(similar)', len(similar))\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    print(\"score\", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
