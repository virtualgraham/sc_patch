{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import ipyplot\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import compress\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import ipyplot\n",
    "import gensim\n",
    "from ast import literal_eval\n",
    "import pathlib\n",
    "\n",
    "from MoCoFeatureExtractor import MoCoFeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and Utility methods for extracting patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'e'\n",
    "\n",
    "window_size = 224\n",
    "image_sizes = [224, 448, 672]\n",
    "stride = 112\n",
    "feature_dim = 2048\n",
    "\n",
    "cnn = MoCoFeatureExtractor(params_path='/Users/racoon/Desktop/moco_v2_800ep_pretrain.pth.tar')\n",
    "image_files = glob(\"dataset_1000/train/*/*.jpg\")[:3]\n",
    "\n",
    "\n",
    "def extract_windows(frame, pos, window_size):\n",
    "    windows = np.empty((len(pos), window_size, window_size, 3), dtype=np.uint8)\n",
    "\n",
    "    for i in range(len(pos)):\n",
    "        windows[i] = extract_window(frame, pos[i], window_size)\n",
    "\n",
    "    return windows\n",
    "\n",
    "\n",
    "def extract_window(frame, pos, window_size):\n",
    "    half_w = window_size/2.0\n",
    "\n",
    "    top_left = [int(round(pos[0]-half_w)), int(round(pos[1]-half_w))]\n",
    "    bottom_right = [top_left[0]+window_size, top_left[1]+window_size]\n",
    "\n",
    "    return frame[top_left[0]:bottom_right[0], top_left[1]:bottom_right[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample and cluster patches\n",
    "This simply uses a fixed grid system. Future patch sampling methods could incorporate an intrest point detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15537 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (224, 299, 3)\n",
      "(1, 1) (56, 94)\n",
      "[(112.0, 150.0)]\n",
      "image.shape (448, 597, 3)\n",
      "(3, 4) (56, 74)\n",
      "[(112.0, 130.0), (112.0, 242.0), (112.0, 354.0), (112.0, 466.0), (224.0, 130.0), (224.0, 242.0), (224.0, 354.0), (224.0, 466.0), (336.0, 130.0), (336.0, 242.0), (336.0, 354.0), (336.0, 466.0)]\n",
      "image.shape (672, 896, 3)\n",
      "(5, 7) (56, 56)\n",
      "[(112.0, 112.0), (112.0, 224.0), (112.0, 336.0), (112.0, 448.0), (112.0, 560.0), (112.0, 672.0), (112.0, 784.0), (224.0, 112.0), (224.0, 224.0), (224.0, 336.0), (224.0, 448.0), (224.0, 560.0), (224.0, 672.0), (224.0, 784.0), (336.0, 112.0), (336.0, 224.0), (336.0, 336.0), (336.0, 448.0), (336.0, 560.0), (336.0, 672.0), (336.0, 784.0), (448.0, 112.0), (448.0, 224.0), (448.0, 336.0), (448.0, 448.0), (448.0, 560.0), (448.0, 672.0), (448.0, 784.0), (560.0, 112.0), (560.0, 224.0), (560.0, 336.0), (560.0, 448.0), (560.0, 560.0), (560.0, 672.0), (560.0, 784.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/15537 [00:07<33:22:54,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['224', '448', '672'])\n",
      "image.shape (224, 288, 3)\n",
      "(1, 1) (56, 88)\n",
      "[(112.0, 144.0)]\n",
      "image.shape (448, 575, 3)\n",
      "(3, 4) (56, 64)\n",
      "[(112.0, 120.0), (112.0, 232.0), (112.0, 344.0), (112.0, 456.0), (224.0, 120.0), (224.0, 232.0), (224.0, 344.0), (224.0, 456.0), (336.0, 120.0), (336.0, 232.0), (336.0, 344.0), (336.0, 456.0)]\n",
      "image.shape (672, 863, 3)\n",
      "(5, 6) (56, 96)\n",
      "[(112.0, 152.0), (112.0, 264.0), (112.0, 376.0), (112.0, 488.0), (112.0, 600.0), (112.0, 712.0), (224.0, 152.0), (224.0, 264.0), (224.0, 376.0), (224.0, 488.0), (224.0, 600.0), (224.0, 712.0), (336.0, 152.0), (336.0, 264.0), (336.0, 376.0), (336.0, 488.0), (336.0, 600.0), (336.0, 712.0), (448.0, 152.0), (448.0, 264.0), (448.0, 376.0), (448.0, 488.0), (448.0, 600.0), (448.0, 712.0), (560.0, 152.0), (560.0, 264.0), (560.0, 376.0), (560.0, 488.0), (560.0, 600.0), (560.0, 712.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/15537 [00:13<59:29:19, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR cnn.evalRGB dataset_1000/train/cat/a3d9e0aab377b038.jpg (672, 863, 3) (30, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9a61b318130e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevalRGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR cnn.evalRGB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/sc_patch/src/evaluation_metric/MoCoFeatureExtractor.py\u001b[0m in \u001b[0;36mevalRGB\u001b[0;34m(self, patches)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    413\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 415\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    416\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, image_file in tqdm(enumerate(image_files), total=len(image_files)):\n",
    "    \n",
    "    pil_image = Image.open(image_file).convert('RGB')\n",
    "    \n",
    "    image_grids = {}\n",
    "    \n",
    "    for image_size in image_sizes:\n",
    "        if pil_image.size[1] > pil_image.size[0]:\n",
    "            resized_image = pil_image.resize((image_size, int(round(pil_image.size[1]/pil_image.size[0] * image_size))))\n",
    "        else:\n",
    "            resized_image = pil_image.resize((int(round(pil_image.size[0]/pil_image.size[1] * image_size)), image_size))\n",
    "        \n",
    "        image = np.array(resized_image)\n",
    "        \n",
    "        margin = window_size-stride\n",
    "        grid_shape = (math.floor((image.shape[0] - margin) / stride), math.floor((image.shape[1] - margin) / stride))\n",
    "        offsets = (round((image.shape[0] - grid_shape[0] * stride)/2), round((image.shape[1] - grid_shape[1] * stride)/2))\n",
    "\n",
    "        points = [(offsets[0]+y*stride+stride/2,offsets[1]+x*stride+stride/2) for y in range(grid_shape[0]) for x in range(grid_shape[1])]\n",
    "        \n",
    "        patches = extract_windows(image, points, window_size)\n",
    "        windows = patches.astype(np.float64)\n",
    "\n",
    "        try:\n",
    "            feats = cnn.evalRGB(windows)\n",
    "        except:\n",
    "            print(\"ERROR cnn.evalRGB\", image_file, image.shape, windows.shape)\n",
    "            raise\n",
    "\n",
    "        feat_grid = feats.reshape((grid_shape[0], grid_shape[1], feature_dim))\n",
    "        \n",
    "        image_grids[str(image_size)] = feat_grid\n",
    "    \n",
    "    path_parts = image_file.split('/')\n",
    "    image_id = path_parts[-1].split('.')[0]\n",
    "    image_class = path_parts[-2]\n",
    "\n",
    "    pathlib.Path(f'feat_grids_{window_size}_{stride}_{version}/{image_class}').mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(f'feat_grids_{window_size}_{stride}_{version}/{image_class}/{image_id}.npz', **image_grids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_files = glob(f'/data/feat_grids_{window_size_outer}_{window_size_inner}_{stride}_{version}/*/*.npz')\n",
    "\n",
    "X = []\n",
    "\n",
    "for idx, npz_file in tqdm(enumerate(npz_files), total=len(npz_files)):\n",
    "\n",
    "    loaded = np.load(npz_file)\n",
    "\n",
    "    feat_grid_outer = loaded['outer']\n",
    "\n",
    "    grid_locs = [(y,x) for y in range(feat_grid_outer.shape[0])for x in range(feat_grid_outer.shape[1])]\n",
    "    \n",
    "    grid_locs = random.sample(grid_locs, cluster_patches_per_image) if len(grid_locs) > cluster_patches_per_image else grid_locs\n",
    "    \n",
    "    for l in grid_locs:\n",
    "        X.append(feat_grid_outer[l])\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Clustering with KMeans: len(X)\", len(X))\n",
    "\n",
    "clusters = KMeans(n_clusters=n_clusters, verbose=False)\n",
    "clusters.fit(np.array(X, dtype=np.float32))\n",
    "\n",
    "pickle.dump(clusters, open(cluster_file, \"wb\"))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pickle.load(open(cluster_file, \"rb\"))\n",
    "\n",
    "npz_files = glob(f'/data/feat_grids_{window_size_outer}_{window_size_inner}_{stride}_{version}/*/*.npz')\n",
    "\n",
    "sub_clusters = [KMeans(n_clusters=10) for _ in range(100)]\n",
    "\n",
    "for c in range(100):\n",
    "    Y = []\n",
    "    \n",
    "    for idx, npz_file in tqdm(enumerate(npz_files), total=len(npz_files)):\n",
    "        \n",
    "        loaded = np.load(npz_file)\n",
    "        feat_grid_outer = loaded['outer']\n",
    "        feat_grid_inner = loaded['inner']\n",
    "\n",
    "        grid_cluster_ids = clusters.predict(feat_grid_outer.reshape(feat_grid_outer.shape[0]*feat_grid_outer.shape[1], feat_grid_outer.shape[2]))\n",
    "        grid_cluster_ids = grid_cluster_ids.reshape((feat_grid_outer.shape[0], feat_grid_outer.shape[1]))\n",
    "\n",
    "        for y in range(feat_grid_outer.shape[0]): \n",
    "            for x in range(feat_grid_outer.shape[1]): \n",
    "                if grid_cluster_ids[y,x] == c:\n",
    "                    Y.append(feat_grid_inner[y,x])\n",
    "    \n",
    "    print('len(Y)', len(Y))\n",
    "    \n",
    "    sub_clusters[c].fit(np.array(Y, dtype=np.float32))\n",
    "    pickle.dump(sub_clusters, open(f'sub_clusters/sub_cluster_{c}.pkl', \"wb\"))\n",
    "    \n",
    "pickle.dump(sub_clusters, open(\"sub_clusers.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_outer = pickle.load(open(cluster_file, \"rb\"))\n",
    "clusters_inner = pickle.load(open(sub_clusters, \"rb\"))\n",
    "     \n",
    "npz_files = glob(f'/data/feat_grids_{window_size_outer}_{window_size_inner}_{stride}_{version}/*/*.npz')[:10]\n",
    "\n",
    "def generate_image_sequences(npz_file):\n",
    "\n",
    "    loaded = np.load(npz_file)\n",
    "    \n",
    "    feat_grid_outer = loaded['outer']\n",
    "    feat_grid_inner = loaded['inner']\n",
    "        \n",
    "    grid_cluster_ids_outer = clusters_outer.predict(feat_grid_outer.reshape(feat_grid_outer.shape[0]*feat_grid_outer.shape[1], feat_grid_outer.shape[2]))\n",
    "    grid_cluster_ids_outer = grid_cluster_ids_outer.reshape((feat_grid_outer.shape[0], feat_grid_outer.shape[1]))\n",
    "\n",
    "    grid_cluster_ids_inner = clusters_inner.predict(feat_grid_inner.reshape(feat_grid_inner.shape[0]*feat_grid_inner.shape[1], feat_grid_inner.shape[2]))\n",
    "    grid_cluster_ids_inner = grid_cluster_ids_inner.reshape((feat_grid_inner.shape[0], feat_grid_inner.shape[1]))\n",
    "    \n",
    "    grid_cluster_ids = grid_cluster_ids_outer * 10 + grid_cluster_ids_inner\n",
    "    \n",
    "    seqs = []\n",
    "    \n",
    "    grid_locations_set = set([(y,x) for y in range(grid_cluster_ids.shape[0]) for x in range(grid_cluster_ids.shape[1])])\n",
    "    \n",
    "    for i in range(walks_per_image):\n",
    "        cluster_seq = []\n",
    "\n",
    "        pos = None\n",
    "        \n",
    "        for t in range(walk_length):\n",
    "            pos = next_pos(grid_locations_set, image_cluster_grid.shape, pos)            \n",
    "            cluster_seq.append(grid_cluster_ids[pos])\n",
    "            \n",
    "        seqs.append([word_format_string.format(w) for w in cluster_seq])\n",
    "        \n",
    "    return seqs\n",
    "\n",
    "\n",
    "cluster_seqs = []\n",
    "image_file_colummn = []\n",
    "\n",
    "for idx, npz_file in tqdm(enumerate(npz_files), total=len(npz_files)):\n",
    "    c_seqs = generate_image_sequences(npz_file)\n",
    "    if c_seqs is None:\n",
    "        continue\n",
    "    cluster_seqs.extend(c_seqs)\n",
    "    image_file_colummn.extend([image_file] * walks_per_image)\n",
    "    \n",
    "data_frame = pd.DataFrame({'words':cluster_seqs, 'file':image_file_colummn})\n",
    "\n",
    "data_frame.to_csv(sequences_file)\n",
    "\n",
    "print(\"done\", len(cluster_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_cluster_grids = np.load(image_cluster_grid_file, allow_pickle=True).item()\n",
    "\n",
    "cluster_seqs = []\n",
    "image_file_colummn = []\n",
    "\n",
    "image_files = list(image_cluster_grids.keys())\n",
    "\n",
    "for idx, image_file in tqdm(enumerate(image_files), total=len(image_files)):\n",
    "    c_seqs = generate_image_sequences(image_cluster_grids[image_file])\n",
    "    if c_seqs is None:\n",
    "        continue\n",
    "    cluster_seqs.extend(c_seqs)\n",
    "    image_file_colummn.extend([image_file] * walks_per_image)\n",
    "\n",
    "data_frame = pd.DataFrame({'words':cluster_seqs, 'file':image_file_colummn})\n",
    "\n",
    "data_frame.to_csv(sequences_file)\n",
    "\n",
    "print(\"done\", len(cluster_seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sequence dataset with random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_locations(grid_shape, stride, offsets, mask=None):\n",
    "    \n",
    "    if mask is not None:\n",
    "        object_grid_locations = set()\n",
    "\n",
    "        for y in range(grid_shape[0]):\n",
    "            for x in range(grid_shape[1]):\n",
    "                p = (offsets[0] + y * stride + 0.5 * stride, offsets[1] + x * stride + 0.5 * stride)\n",
    "                w = extract_window(mask, p, stride)\n",
    "\n",
    "                if np.sum(w) >= stride * stride * 0.3:\n",
    "                    object_grid_locations.add((y, x))\n",
    "        \n",
    "        return object_grid_locations\n",
    "    \n",
    "    else:\n",
    "        return [(y,x) for y in range(grid_shape[0]) for x in range(grid_shape[1])]\n",
    "    \n",
    "    \n",
    "def generate_image_cluster_grid(image_file, image_scale, clusters, feature_extractor):\n",
    "    # print(\"generate_image_sequences\", image_file)\n",
    "\n",
    "    pil_image = Image.open(image_file).convert('RGB')\n",
    "    pil_image = pil_image.resize((int(round(pil_image.size[0] * image_scale)), int(round(pil_image.size[1] * image_scale))))\n",
    "    image = np.array(pil_image)\n",
    "\n",
    "    if image.shape[0] < window_size * 2 or image.shape[1] < window_size * 2:\n",
    "        print(\"image too small, image_file\")\n",
    "        return None\n",
    "            \n",
    "    margin = window_size-stride\n",
    "    grid_shape = (math.floor((image.shape[0] - margin) / stride), math.floor((image.shape[1] - margin) / stride))\n",
    "    offsets = (round((image.shape[0] - grid_shape[0] * stride)/2), round((image.shape[1] - grid_shape[1] * stride)/2))\n",
    "\n",
    "    grid_locations_set = get_grid_locations(grid_shape, stride, offsets)\n",
    "    grid_locations_list = list(grid_locations_set)\n",
    "    \n",
    "    points = [(y*stride + stride/2 + offsets[0], x*stride + stride/2 + offsets[1]) for (y,x) in grid_locations_list]\n",
    "        \n",
    "    patches = extract_windows(image, points, window_size)\n",
    "    windows = patches.astype(np.float64)\n",
    "\n",
    "    # print(windows.shape)\n",
    "    \n",
    "    feats = cnn.evalRGB(windows)\n",
    "    feats = feats.reshape((windows.shape[0], feature_dim))\n",
    "\n",
    "    grid_cluster_ids = clusters.predict(feats)\n",
    "        \n",
    "    cluster_grid = np.full(grid_shape, -1, dtype=int)\n",
    "    \n",
    "    for i in range(len(grid_locations_list)):\n",
    "        cluster_grid[grid_locations_list[i]] = grid_cluster_ids[i]\n",
    "        \n",
    "    return cluster_grid\n",
    "\n",
    "\n",
    "def generate_image_sequences(image_cluster_grid, seq_count=walks_per_image):\n",
    "\n",
    "    cluster_seqs = []\n",
    "    \n",
    "    grid_locations_set = set([(y,x) for y in range(image_cluster_grid.shape[0]) for x in range(image_cluster_grid.shape[1])])\n",
    "    \n",
    "    for i in range(seq_count):\n",
    "        cluster_seq = []\n",
    "\n",
    "        pos = None\n",
    "        \n",
    "        for t in range(walk_length):\n",
    "            pos = next_pos(grid_locations_set, image_cluster_grid.shape, pos)\n",
    "            cluster_seq.append(image_cluster_grid[pos])\n",
    "            \n",
    "        cluster_seqs.append([word_format_string.format(w) for w in cluster_seq])\n",
    "        \n",
    "    return cluster_seqs\n",
    "\n",
    "\n",
    "def mask_locations(mask, stride, grid_shape, offsets):\n",
    "    \n",
    "    object_grid_locations = set()\n",
    "\n",
    "    for y in range(grid_shape[0]):\n",
    "        for x in range(grid_shape[1]):\n",
    "            p = (offsets[0] + y * stride + 0.5 * stride, offsets[1] + x * stride + 0.5 * stride)\n",
    "            w = extract_window(mask, p, stride)\n",
    "\n",
    "            # print(np.sum(w))\n",
    "            if np.sum(w) >= stride * stride * 0.3:\n",
    "                object_grid_locations.add((y, x))\n",
    "\n",
    "    return object_grid_locations\n",
    "\n",
    "     \n",
    "def generate_masked_image_sequences(image_file, mask_file, clusters, feature_extractor, seq_count=walks_per_image):\n",
    "\n",
    "    pil_image = Image.open(image_file).convert('RGB')\n",
    "    pil_image = pil_image.resize((int(round(pil_image.size[0] * image_scale)), int(round(pil_image.size[1] * image_scale))))\n",
    "    image = np.array(pil_image)\n",
    "\n",
    "    if image.shape[0] < window_size * 2 or image.shape[1] < window_size * 2:\n",
    "        print(\"image too small, image_file\")\n",
    "        return None\n",
    "            \n",
    "    pil_mask = Image.open(mask_file).convert('1')\n",
    "    pil_mask = pil_mask.resize((int(round(pil_mask.size[0] * image_scale)), int(round(pil_mask.size[1] * image_scale))))\n",
    "    mask = np.array(pil_mask)\n",
    "        \n",
    "    margin = window_size-stride\n",
    "    grid_shape = (math.floor((image.shape[0] - margin) / stride), math.floor((image.shape[1] - margin) / stride))\n",
    "    offsets = (round((image.shape[0] - grid_shape[0] * stride)/2), round((image.shape[1] - grid_shape[1] * stride)/2))\n",
    "\n",
    "    grid_locations_set = mask_locations(mask, stride, grid_shape, offsets)\n",
    "    grid_locations_list = list(grid_locations_set)\n",
    "    \n",
    "    points = [(y*stride + stride/2 + offsets[0], x*stride + stride/2 + offsets[1]) for (y,x) in grid_locations_list]\n",
    "        \n",
    "    patches = extract_windows(image, points, window_size)\n",
    "    windows = patches.astype(np.float64)\n",
    "\n",
    "    feats = cnn.evalRGB(windows)\n",
    "    feats = feats.reshape((windows.shape[0], feature_dim))\n",
    "\n",
    "    grid_cluster_ids = clusters.predict(feats)\n",
    "    grid_location_to_cluster_id = dict([(grid_locations_list[i], grid_cluster_ids[i]) for i in range(len(grid_locations_list))])\n",
    "          \n",
    "    cluster_seqs = []\n",
    "    for i in range(seq_count):\n",
    "        cluster_seq = []\n",
    "        \n",
    "        pos = None\n",
    "        \n",
    "        for t in range(walk_length):\n",
    "            pos = next_pos(grid_locations_set, grid_shape, pos)\n",
    "            cluster_seq.append(grid_location_to_cluster_id[pos])\n",
    "            \n",
    "        cluster_seqs.append([word_format_string.format(w) for w in cluster_seq])\n",
    "  \n",
    "    return cluster_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pickle.load(open(cluster_file, \"rb\"))\n",
    "\n",
    "image_cluster_grids = {}\n",
    "\n",
    "for idx, image_file in tqdm(enumerate(image_files), total=len(image_files)):\n",
    "\n",
    "    image_cluster_grid = generate_image_cluster_grid(image_file, image_scale, clusters, cnn)\n",
    "    if image_cluster_grid is None:\n",
    "        continue\n",
    "    image_id = image_file.split('/')[-1].split('.')[0]\n",
    "    image_cluster_grids[image_id] = image_cluster_grid    \n",
    "    \n",
    "np.save(image_cluster_grid_file, image_cluster_grids)   \n",
    "\n",
    "print(\"done\")\n",
    "# print(np.load(image_cluster_grid_file, allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_cluster_grids = np.load(image_cluster_grid_file, allow_pickle=True).item()\n",
    "\n",
    "cluster_seqs = []\n",
    "image_file_colummn = []\n",
    "\n",
    "image_files = list(image_cluster_grids.keys())\n",
    "\n",
    "for idx, image_file in tqdm(enumerate(image_files), total=len(image_files)):\n",
    "    c_seqs = generate_image_sequences(image_cluster_grids[image_file])\n",
    "    if c_seqs is None:\n",
    "        continue\n",
    "    cluster_seqs.extend(c_seqs)\n",
    "    image_file_colummn.extend([image_file] * walks_per_image)\n",
    "\n",
    "data_frame = pd.DataFrame({'words':cluster_seqs, 'file':image_file_colummn})\n",
    "\n",
    "data_frame.to_csv(sequences_file)\n",
    "\n",
    "print(\"done\", len(cluster_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print('epoch {}'.format(self.epoch))\n",
    "        self.epoch += 1\n",
    "              \n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    data_frame = pd.read_csv(sequences_file,converters={\"words\": literal_eval})\n",
    "    \n",
    "    for index, row in data_frame.iterrows():\n",
    "        if tokens_only:\n",
    "            yield row['words']\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(row['words'], [index])\n",
    "\n",
    "train_corpus = list(read_corpus(sequences_file))\n",
    "print(train_corpus[:2])\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=256, epochs=30)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs, callbacks=[callback()])\n",
    "\n",
    "model.save(doc2vec_file)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pickle.load(open(cluster_file, \"rb\"))\n",
    "model = gensim.models.doc2vec.Doc2Vec.load(doc2vec_file)\n",
    "\n",
    "data_frame = pd.read_csv(sequences_file, converters={\"words\": literal_eval})\n",
    "\n",
    "test_image_files = glob(\"dataset_100/test/*/*.jpg\")\n",
    "test_mask_files = glob(\"dataset_100/test/*/*.mask.png\")\n",
    "\n",
    "test_image_files.sort()\n",
    "test_mask_files.sort()\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(len(test_image_files)):\n",
    "    \n",
    "    image_correct = 0\n",
    "    image_total = 0\n",
    "    \n",
    "    image_file = test_image_files[i]\n",
    "    mask_file = test_mask_files[i]\n",
    "\n",
    "    print(\"test\", image_file)\n",
    "\n",
    "    c_seqs = generate_masked_image_sequences(image_file, mask_file, clusters, cnn, seq_count=100)\n",
    "\n",
    "    if c_seqs is None:\n",
    "        continue\n",
    "        \n",
    "    vectors = [[model.infer_vector(s)] for s in c_seqs]\n",
    "\n",
    "    for v in vectors:\n",
    "        similar = model.docvecs.most_similar(v, topn=10)\n",
    "        #print('similar', similar)\n",
    "\n",
    "        for s in similar:\n",
    "            f = data_frame.loc[s[0],'file']\n",
    "            \n",
    "            a = image_file.split('/')[-2]\n",
    "            b = image_id_to_class[f]\n",
    "           \n",
    "            if a == b:\n",
    "                image_correct += 1\n",
    "                correct += 1\n",
    "           \n",
    "            image_total += 1\n",
    "            total += 1\n",
    "            \n",
    "    print(\"score\", image_correct/image_total)\n",
    "    \n",
    "print(\"final score\", correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "```\n",
    "96  72  1000 0.36  \n",
    "96  72  100  0.45  \n",
    "192 72  100  0.77\n",
    "288 144 100  0.82\n",
    "288 144 1000 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
