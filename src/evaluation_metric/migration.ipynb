{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import math\n",
    "import random \n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def rsetattr(obj, attr, val):\n",
    "    pre, _, post = attr.rpartition('.')\n",
    "    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n",
    "\n",
    "def rgetattr(obj, attr, *args):\n",
    "    def _getattr(obj, attr):\n",
    "        return getattr(obj, attr, *args)\n",
    "    return functools.reduce(_getattr, [obj] + attr.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Model for Evaluation\n",
    "##################################################\n",
    "\n",
    "class ShufflePatchEvalNet(nn.Module):\n",
    "  def __init__(self,aux_logits = False):\n",
    "\n",
    "      super(ShufflePatchEvalNet, self).__init__()\n",
    "\n",
    "      self.cnn = nn.Sequential(\n",
    "        nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(128), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(128), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "      )\n",
    "    \n",
    "  def forward(self, patch):\n",
    "    return self.cnn(patch)\n",
    "\n",
    "\n",
    "# https://github.com/pratogab/batch-transforms/blob/master/batch_transforms.py\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\"Applies the :class:`~torchvision.transforms.ToTensor` transform to a batch of images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.max = 255\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor of size (N, C, H, W) to be tensorized.\n",
    "        Returns:\n",
    "            Tensor: Tensorized Tensor.\n",
    "        \"\"\"\n",
    "        return tensor.float().div_(self.max)\n",
    "\n",
    "\n",
    "class Normalize:\n",
    "    \"\"\"Applies the :class:`~torchvision.transforms.Normalize` transform to a batch of images.\n",
    "    .. note::\n",
    "        This transform acts out of place by default, i.e., it does not mutate the input tensor.\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "        inplace(bool,optional): Bool to make this operation in-place.\n",
    "        dtype (torch.dtype,optional): The data type of tensors to which the transform will be applied.\n",
    "        device (torch.device,optional): The device of tensors to which the transform will be applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std, inplace=False, dtype=torch.float, device='cpu'):\n",
    "        self.mean = torch.as_tensor(mean, dtype=dtype, device=device)[None, :, None, None]\n",
    "        self.std = torch.as_tensor(std, dtype=dtype, device=device)[None, :, None, None]\n",
    "        self.inplace = inplace\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor of size (N, C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor.\n",
    "        \"\"\"\n",
    "        if not self.inplace:\n",
    "            tensor = tensor.clone()\n",
    "\n",
    "        tensor.sub_(self.mean).div_(self.std)\n",
    "        return tensor\n",
    "\n",
    "    \n",
    "class ShufflePatchFeatureExtractor():\n",
    "    def __init__(self, weights_path):\n",
    "        self.model = ShufflePatchEvalNet().to(device)\n",
    "        \n",
    "        print('Loading Weights...', weights_path)\n",
    "        checkpoint = torch.load(weights_path, map_location=torch.device(device))\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        self.transform_batch = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
    "        )\n",
    "               \n",
    "    # Numpy array of size (N, H, W, C)\n",
    "    # Used for PIL images\n",
    "    def evalRGB(self, patches):\n",
    "        patches = torch.from_numpy(patches)\n",
    "        patches = patches.permute(0, 3, 1, 2)\n",
    "        patches = self.transform_batch(patches)\n",
    "        output = self.model(patches.to(device))\n",
    "        return output.cpu().detach().numpy()\n",
    "\n",
    "    # Numpy array of size (N, H, W, C)\n",
    "    # Used for CV2 images\n",
    "    def evalBGR(self, patches):\n",
    "        patches = patches[...,::-1].copy()\n",
    "        return self.evalRGB(patches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint... /Users/racoon/Desktop/rotation_jigsaw_0820_0.0001_1.9327_43.59.pt\n",
      "odict_keys(['cnn.0.weight', 'cnn.0.bias', 'cnn.1.weight', 'cnn.1.bias', 'cnn.1.running_mean', 'cnn.1.running_var', 'cnn.1.num_batches_tracked', 'cnn.3.weight', 'cnn.3.bias', 'cnn.4.weight', 'cnn.4.bias', 'cnn.4.running_mean', 'cnn.4.running_var', 'cnn.4.num_batches_tracked', 'cnn.7.weight', 'cnn.7.bias', 'cnn.8.weight', 'cnn.8.bias', 'cnn.8.running_mean', 'cnn.8.running_var', 'cnn.8.num_batches_tracked', 'cnn.10.weight', 'cnn.10.bias', 'cnn.11.weight', 'cnn.11.bias', 'cnn.11.running_mean', 'cnn.11.running_var', 'cnn.11.num_batches_tracked', 'cnn.14.weight', 'cnn.14.bias', 'cnn.15.weight', 'cnn.15.bias', 'cnn.15.running_mean', 'cnn.15.running_var', 'cnn.15.num_batches_tracked', 'cnn.17.weight', 'cnn.17.bias', 'cnn.18.weight', 'cnn.18.bias', 'cnn.18.running_mean', 'cnn.18.running_var', 'cnn.18.num_batches_tracked', 'cnn.20.weight', 'cnn.20.bias', 'cnn.21.weight', 'cnn.21.bias', 'cnn.21.running_mean', 'cnn.21.running_var', 'cnn.21.num_batches_tracked', 'cnn.24.weight', 'cnn.24.bias', 'cnn.25.weight', 'cnn.25.bias', 'cnn.25.running_mean', 'cnn.25.running_var', 'cnn.25.num_batches_tracked', 'cnn.27.weight', 'cnn.27.bias', 'cnn.28.weight', 'cnn.28.bias', 'cnn.28.running_mean', 'cnn.28.running_var', 'cnn.28.num_batches_tracked', 'cnn.30.weight', 'cnn.30.bias', 'cnn.31.weight', 'cnn.31.bias', 'cnn.31.running_mean', 'cnn.31.running_var', 'cnn.31.num_batches_tracked', 'cnn.34.weight', 'cnn.34.bias', 'cnn.35.weight', 'cnn.35.bias', 'cnn.35.running_mean', 'cnn.35.running_var', 'cnn.35.num_batches_tracked', 'cnn.37.weight', 'cnn.37.bias', 'cnn.38.weight', 'cnn.38.bias', 'cnn.38.running_mean', 'cnn.38.running_var', 'cnn.38.num_batches_tracked', 'cnn.40.weight', 'cnn.40.bias', 'cnn.41.weight', 'cnn.41.bias', 'cnn.41.running_mean', 'cnn.41.running_var', 'cnn.41.num_batches_tracked', 'fc6.0.weight', 'fc6.0.bias', 'fc.0.weight', 'fc.0.bias', 'fc.3.weight', 'fc.3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# Migrate Model\n",
    "\n",
    "model = ShufflePatchEvalNet().to(device)\n",
    "\n",
    "model_save_path = \"/Users/racoon/Desktop/rotation_jigsaw_0820_0.0001_1.9327_43.59.pt\"\n",
    "\n",
    "print('Loading Checkpoint...', model_save_path)\n",
    "\n",
    "checkpoint = torch.load(model_save_path, map_location=torch.device(device))\n",
    "\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "print(state_dict.keys())\n",
    "\n",
    "named_parameters = set(model.named_parameters())\n",
    "\n",
    "with torch.no_grad():\n",
    "    for k in state_dict.keys():\n",
    "        if k in named_parameters: # k.startswith('cnn.'):\n",
    "            print(k)\n",
    "            rgetattr(model, k).copy_(state_dict[k])\n",
    "\n",
    "model_save_path = \"/Users/racoon/Desktop/rotation_jigsaw_migrated_0820_0.0001_1.9327_43.59.pt\"\n",
    "\n",
    "torch.save(\n",
    "{\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint... /Users/racoon/Desktop/variation_2b_migrated_0135_0.001_1.4328_63.80.pt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d7f1bf40eed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading Checkpoint...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Migrated Model\n",
    "model_save_path = \"/Users/racoon/Desktop/variation_2b_migrated_0135_0.001_1.4328_63.80.pt\"\n",
    "print('Loading Checkpoint...', model_save_path)\n",
    "checkpoint = torch.load(model_save_path, map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_dim = 32\n",
    "validation_image_paths = glob('/Users/racoon/Downloads/open-images-sample/*.jpg')\n",
    "reuse_image_count = 0\n",
    "\n",
    "class ShufflePatchDataset(Dataset):\n",
    "\n",
    "  def __init__(self, image_paths, patch_dim, length, transform=None):\n",
    "    self.image_paths = image_paths\n",
    "    self.patch_dim = patch_dim\n",
    "    self.length = length\n",
    "    self.transform = transform\n",
    "    self.image_reused = 0\n",
    "    self.min_width = self.patch_dim\n",
    "    \n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    # [y, x, chan], dtype=uint8, top_left is (0,0)\n",
    "    \n",
    "    image_index = int(math.floor((len(self.image_paths) * random.random())))\n",
    "    \n",
    "    if self.image_reused == 0:\n",
    "      pil_image = Image.open(self.image_paths[image_index]).convert('RGB')\n",
    "      self.pil_image = pil_image.resize((int(round(pil_image.size[0]/3)), int(round(pil_image.size[1]/3))))\n",
    "      self.image_reused = reuse_image_count\n",
    "    else:\n",
    "      self.image_reused -= 1\n",
    "\n",
    "    image = np.array(self.pil_image)\n",
    "\n",
    "    # If image is too small, try another image\n",
    "    if (image.shape[0] - self.min_width) <= 0 or (image.shape[1] - self.min_width) <= 0:\n",
    "        return self.__getitem__(index)\n",
    "    \n",
    "    y_coord = int(math.floor((image.shape[0] - self.patch_dim) * random.random()))\n",
    "    x_coord = int(math.floor((image.shape[1] - self.patch_dim) * random.random()))\n",
    "\n",
    "    patch = image[y_coord:y_coord+self.patch_dim, x_coord:x_coord+self.patch_dim]\n",
    "\n",
    "    if self.transform:\n",
    "      patch = self.transform(patch)\n",
    "\n",
    "    return patch\n",
    "\n",
    "\n",
    "valdataset = ShufflePatchDataset(validation_image_paths, patch_dim, 1,\n",
    "                         transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valdataset,\n",
    "                                        batch_size=1,\n",
    "                                        num_workers=0,\n",
    "                                        shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f5ea4d85f775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimg0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "feat.model.eval()\n",
    "\n",
    "preprocess_input =  transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "data = next(iter(valloader))\n",
    "\n",
    "\n",
    "img0 = data.to(device)\n",
    "output = feat.model(img0)\n",
    "output = output.cpu().detach().numpy()\n",
    "print(output.shape)\n",
    "output = output.reshape((512,))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
