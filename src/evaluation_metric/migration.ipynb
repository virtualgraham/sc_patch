{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import math\n",
    "import random \n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def rsetattr(obj, attr, val):\n",
    "    pre, _, post = attr.rpartition('.')\n",
    "    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n",
    "\n",
    "def rgetattr(obj, attr, *args):\n",
    "    def _getattr(obj, attr):\n",
    "        return getattr(obj, attr, *args)\n",
    "    return functools.reduce(_getattr, [obj] + attr.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Model for Evaluation\n",
    "##################################################\n",
    "\n",
    "class ShufflePatchEvalNet(nn.Module):\n",
    "  def __init__(self,aux_logits = False):\n",
    "\n",
    "      super(ShufflePatchEvalNet, self).__init__()\n",
    "\n",
    "      self.cnn = nn.Sequential(\n",
    "        nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(128), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(128), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(512), \n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "      )\n",
    "    \n",
    "  def forward(self, patch):\n",
    "    return self.cnn(patch)\n",
    "\n",
    "\n",
    "# https://github.com/pratogab/batch-transforms/blob/master/batch_transforms.py\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\"Applies the :class:`~torchvision.transforms.ToTensor` transform to a batch of images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.max = 255\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor of size (N, C, H, W) to be tensorized.\n",
    "        Returns:\n",
    "            Tensor: Tensorized Tensor.\n",
    "        \"\"\"\n",
    "        return tensor.float().div_(self.max)\n",
    "\n",
    "\n",
    "class Normalize:\n",
    "    \"\"\"Applies the :class:`~torchvision.transforms.Normalize` transform to a batch of images.\n",
    "    .. note::\n",
    "        This transform acts out of place by default, i.e., it does not mutate the input tensor.\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "        inplace(bool,optional): Bool to make this operation in-place.\n",
    "        dtype (torch.dtype,optional): The data type of tensors to which the transform will be applied.\n",
    "        device (torch.device,optional): The device of tensors to which the transform will be applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std, inplace=False, dtype=torch.float, device='cpu'):\n",
    "        self.mean = torch.as_tensor(mean, dtype=dtype, device=device)[None, :, None, None]\n",
    "        self.std = torch.as_tensor(std, dtype=dtype, device=device)[None, :, None, None]\n",
    "        self.inplace = inplace\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor of size (N, C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor.\n",
    "        \"\"\"\n",
    "        if not self.inplace:\n",
    "            tensor = tensor.clone()\n",
    "\n",
    "        tensor.sub_(self.mean).div_(self.std)\n",
    "        return tensor\n",
    "\n",
    "    \n",
    "class ShufflePatchFeatureExtractor():\n",
    "    def __init__(self, weights_path):\n",
    "        self.model = ShufflePatchEvalNet().to(device)\n",
    "        \n",
    "        print('Loading Weights...', weights_path)\n",
    "        checkpoint = torch.load(weights_path, map_location=torch.device(device))\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        self.transform_batch = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
    "        )\n",
    "               \n",
    "    # Numpy array of size (N, H, W, C)\n",
    "    # Used for PIL images\n",
    "    def evalRGB(self, patches):\n",
    "        patches = torch.from_numpy(patches)\n",
    "        patches = patches.permute(0, 3, 1, 2)\n",
    "        patches = self.transform_batch(patches)\n",
    "        output = self.model(patches.to(device))\n",
    "        return output.cpu().detach().numpy()\n",
    "\n",
    "    # Numpy array of size (N, H, W, C)\n",
    "    # Used for CV2 images\n",
    "    def evalBGR(self, patches):\n",
    "        patches = patches[...,::-1].copy()\n",
    "        return self.evalRGB(patches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Weights... /Users/racoon/Desktop/variation_2b_migrated_0135_0.001_1.4328_63.80.pt\n",
      "0.00021076202392578125\n",
      "[False False  True  True  True False False False False  True False False\n",
      "  True False False False False False False False False  True False False\n",
      "  True False False False  True False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False False False False  True  True\n",
      " False False False  True False False False False False False  True False\n",
      "  True False False False False False  True False  True False False False\n",
      "  True False False False  True False False  True False  True False False\n",
      " False False False False False  True  True False  True False False False\n",
      "  True False False False False False False False False False  True False\n",
      " False False False  True False False False False False  True False False\n",
      "  True False False False False False  True False False False False False\n",
      "  True False False False False False False  True False  True False False\n",
      "  True False False False False  True  True False False False False False\n",
      " False False False  True False  True  True False False  True False False\n",
      " False False  True  True  True False False False False  True  True False\n",
      " False False False  True False False False  True False False False  True\n",
      " False False False False False False False  True False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False  True False False False False  True False  True  True False  True\n",
      " False False  True False False False  True False False False False False\n",
      " False False False False False False False  True  True  True False False\n",
      " False False  True False False False  True False  True  True False False\n",
      " False False False  True False False  True False False False False  True\n",
      "  True False False False False False False False False False False  True\n",
      "  True False False  True False False False False False False False False\n",
      " False False  True False False  True False False False  True False False\n",
      " False  True False False False False False False False False False False\n",
      " False False  True False False False  True False False  True False  True\n",
      " False  True False False  True False False  True False False False False\n",
      " False False False False False  True False  True  True False  True False\n",
      " False False False False  True False False False False False False  True\n",
      " False False False False False False False False  True  True False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False  True False False False False False False  True False\n",
      " False False  True False False False False False False False False False\n",
      " False False False  True False False  True False  True False  True  True\n",
      "  True False False False False False  True False False False False False\n",
      " False  True False False False False False False False False  True  True\n",
      " False  True False False False  True  True False False False False False\n",
      " False  True False False False False False  True False False False False\n",
      " False  True False  True  True False False False False  True  True False\n",
      " False  True False False False False False False]\n",
      "[2.21872997e+00 1.05069208e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 2.15524507e+00 3.60413909e-01 1.41171753e+00\n",
      " 2.76199996e-01 0.00000000e+00 8.31166059e-02 1.31934988e+00\n",
      " 0.00000000e+00 4.20042217e-01 1.10045266e+00 2.16414142e+00\n",
      " 8.88882577e-02 9.83299792e-01 6.04766309e-01 3.23338866e-01\n",
      " 1.66324303e-02 0.00000000e+00 1.46050811e+00 2.13266397e+00\n",
      " 0.00000000e+00 2.79062843e+00 2.08238721e+00 9.71354663e-01\n",
      " 0.00000000e+00 1.34782910e+00 1.50853443e+00 4.75073546e-01\n",
      " 1.92232579e-02 1.75986350e-01 9.35541034e-01 9.50238287e-01\n",
      " 2.18818235e+00 1.51087165e+00 2.04072207e-01 3.99345368e-01\n",
      " 1.03508934e-01 9.35296118e-01 5.95199704e-01 1.09594032e-01\n",
      " 0.00000000e+00 7.80839086e-01 1.69299245e-01 2.03042459e+00\n",
      " 2.52283120e+00 7.59679258e-01 8.13242793e-03 1.40127885e+00\n",
      " 1.61781943e+00 6.25653625e-01 1.95446956e+00 6.49516404e-01\n",
      " 2.31285954e+00 1.41169381e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.89042777e-01 1.19039249e+00 6.64694846e-01 0.00000000e+00\n",
      " 3.28544831e+00 1.69983834e-01 1.06127036e+00 2.70027161e-01\n",
      " 7.96478868e-01 1.74678087e+00 0.00000000e+00 1.88217256e-02\n",
      " 0.00000000e+00 1.31449139e+00 1.62001824e+00 5.80131412e-01\n",
      " 1.79596639e+00 1.29750445e-01 0.00000000e+00 4.95504029e-02\n",
      " 0.00000000e+00 1.60917473e+00 1.48810863e+00 3.21671295e+00\n",
      " 0.00000000e+00 5.19486606e-01 7.44933307e-01 1.98767734e+00\n",
      " 0.00000000e+00 4.08932567e-01 5.05958617e-01 0.00000000e+00\n",
      " 4.30465788e-02 0.00000000e+00 1.48016787e+00 3.26940656e-01\n",
      " 1.68116820e+00 2.07732844e+00 1.13455081e+00 3.75025541e-01\n",
      " 2.15399313e+00 0.00000000e+00 0.00000000e+00 2.94376016e+00\n",
      " 0.00000000e+00 1.16779268e+00 4.76067424e-01 2.32618356e+00\n",
      " 0.00000000e+00 1.21076155e+00 7.56439865e-02 2.01087534e-01\n",
      " 9.20595974e-02 3.89935672e-02 3.24536991e+00 2.13302541e+00\n",
      " 1.21571350e+00 8.65153372e-01 0.00000000e+00 6.06306866e-02\n",
      " 1.81392297e-01 2.70514536e+00 6.21904433e-01 0.00000000e+00\n",
      " 1.49577045e+00 6.13259077e-01 4.53474075e-02 4.36002128e-02\n",
      " 4.27463800e-01 0.00000000e+00 0.00000000e+00 2.55421686e+00\n",
      " 0.00000000e+00 2.16044456e-01 1.97078004e-01 1.58280647e+00\n",
      " 9.15183127e-02 1.46380723e+00 0.00000000e+00 1.43151522e+00\n",
      " 1.21298385e+00 9.02132809e-01 1.82152972e-01 4.30588350e-02\n",
      " 0.00000000e+00 3.17696428e+00 4.70600367e-01 2.37646985e+00\n",
      " 2.10259452e-01 1.45252085e+00 2.08176470e+00 0.00000000e+00\n",
      " 2.00840759e+00 0.00000000e+00 2.32768700e-01 2.89479542e+00\n",
      " 0.00000000e+00 1.62548460e-02 2.52185297e+00 1.69119132e+00\n",
      " 3.33129525e-01 0.00000000e+00 0.00000000e+00 1.33990198e-01\n",
      " 3.43181789e-02 5.60228050e-01 1.79965127e+00 2.08092183e-02\n",
      " 1.74871519e-01 1.96019411e+00 1.11105907e+00 0.00000000e+00\n",
      " 3.73795211e-01 0.00000000e+00 0.00000000e+00 6.02047145e-03\n",
      " 1.35643458e+00 0.00000000e+00 1.28542376e+00 6.38340890e-01\n",
      " 1.98693678e-01 1.47934699e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.54518127e+00 4.38650995e-02 2.68276286e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.17563471e-01\n",
      " 2.53814054e+00 3.34699959e-01 1.27667892e+00 0.00000000e+00\n",
      " 1.65126726e-01 6.69505820e-03 2.82917954e-02 0.00000000e+00\n",
      " 2.05248475e+00 2.37832755e-01 2.69237071e-01 0.00000000e+00\n",
      " 2.31493282e+00 1.87601790e-01 1.76997870e-01 3.19959372e-02\n",
      " 7.61316717e-02 2.30473709e+00 7.59745955e-01 0.00000000e+00\n",
      " 1.89670765e+00 2.85818434e+00 1.98214269e+00 5.99434853e-01\n",
      " 5.69125175e-01 2.11609721e-01 0.00000000e+00 9.98281479e-01\n",
      " 1.38407552e+00 6.49505079e-01 3.43937352e-02 4.09758016e-02\n",
      " 1.57003999e-01 2.11725783e+00 7.35687494e-01 2.34638929e+00\n",
      " 2.14876384e-02 0.00000000e+00 1.45809078e+00 6.90200478e-02\n",
      " 1.20542669e+00 4.74892139e-01 0.00000000e+00 3.12990546e-01\n",
      " 0.00000000e+00 0.00000000e+00 1.07782769e+00 0.00000000e+00\n",
      " 3.03112656e-01 1.92727959e+00 0.00000000e+00 5.14900684e-01\n",
      " 1.88591194e+00 8.01657438e-01 0.00000000e+00 1.89668679e+00\n",
      " 1.72924250e-03 2.16888475e+00 2.28913024e-01 9.82654214e-01\n",
      " 1.45603687e-01 3.40755254e-01 6.44529104e-01 3.16291511e-01\n",
      " 2.25403571e+00 2.57916307e+00 2.33332109e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.78272951e-01 1.21818101e+00\n",
      " 6.13207221e-01 2.17073083e-01 0.00000000e+00 2.57618517e-01\n",
      " 1.39524972e+00 4.11252499e-01 0.00000000e+00 1.00101125e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.99007764e-01 1.67050898e+00\n",
      " 1.55695736e-01 2.27533609e-01 1.17791080e+00 0.00000000e+00\n",
      " 1.21777594e+00 3.17462355e-01 0.00000000e+00 1.25254762e+00\n",
      " 1.58612058e-02 1.43572187e+00 1.92015409e-01 0.00000000e+00\n",
      " 0.00000000e+00 2.44536662e+00 2.42271948e+00 2.57875100e-02\n",
      " 3.33072722e-01 5.61961681e-02 7.51711130e-02 5.58746159e-02\n",
      " 1.13773084e+00 1.85147607e+00 1.50994107e-01 0.00000000e+00\n",
      " 0.00000000e+00 1.59975708e+00 2.65524530e+00 0.00000000e+00\n",
      " 1.32604313e+00 9.89971459e-01 3.55838686e-02 1.38604927e+00\n",
      " 1.42871714e+00 1.22954726e+00 5.90484560e-01 9.03407753e-01\n",
      " 0.00000000e+00 2.05673963e-01 0.00000000e+00 9.14357483e-01\n",
      " 2.03045297e+00 0.00000000e+00 6.24715388e-02 2.35287070e-01\n",
      " 1.88493237e-01 0.00000000e+00 1.33565414e+00 6.50760949e-01\n",
      " 2.91712061e-02 0.00000000e+00 5.16451560e-02 3.40005398e-01\n",
      " 3.59513819e-01 8.30019236e-01 1.12571049e+00 4.91501808e-01\n",
      " 1.66659713e+00 2.94949591e-01 8.14625740e-01 1.23493922e+00\n",
      " 1.27820969e-01 1.22517660e-01 0.00000000e+00 2.05112413e-01\n",
      " 1.89817473e-01 1.13614130e+00 0.00000000e+00 8.80472422e-01\n",
      " 9.35277164e-01 0.00000000e+00 2.33509660e+00 0.00000000e+00\n",
      " 2.50635862e+00 0.00000000e+00 2.82294250e+00 1.14094651e+00\n",
      " 0.00000000e+00 5.23367375e-02 1.28040242e+00 0.00000000e+00\n",
      " 1.83339560e+00 1.07991803e+00 1.34379637e+00 6.73582196e-01\n",
      " 6.22867048e-02 3.87386680e-01 3.52342427e-01 2.49947548e-01\n",
      " 4.51233476e-01 0.00000000e+00 6.85540497e-01 0.00000000e+00\n",
      " 0.00000000e+00 1.98590302e+00 0.00000000e+00 3.79704572e-02\n",
      " 3.31970870e-01 2.94995403e+00 1.10420334e+00 7.13545024e-01\n",
      " 0.00000000e+00 3.70380998e-01 1.83347911e-01 2.27774572e+00\n",
      " 3.45445722e-01 2.37910137e-01 2.07440567e+00 0.00000000e+00\n",
      " 2.75664854e+00 1.17195606e+00 9.35988307e-01 2.64721990e+00\n",
      " 2.69443274e+00 1.66543514e-01 5.37703931e-01 1.86422694e+00\n",
      " 0.00000000e+00 0.00000000e+00 7.75651217e-01 0.00000000e+00\n",
      " 1.96700466e+00 2.08467022e-02 5.01000583e-02 3.76014888e-01\n",
      " 5.16360581e-01 9.78425592e-02 5.16005307e-02 7.49921858e-01\n",
      " 2.07791257e+00 1.90765634e-01 2.05315089e+00 8.32667291e-01\n",
      " 3.74851495e-01 4.81469601e-01 4.91629392e-02 0.00000000e+00\n",
      " 1.59565187e+00 2.15469837e-01 1.86782479e-01 9.80595350e-01\n",
      " 3.89577925e-01 1.56906247e-02 0.00000000e+00 2.01732540e+00\n",
      " 1.66605139e+00 1.06786799e+00 0.00000000e+00 6.39323175e-01\n",
      " 2.82834697e+00 3.72741103e-01 2.69952273e+00 8.27749968e-01\n",
      " 2.40115732e-01 4.54523116e-02 7.20252454e-01 1.47157192e+00\n",
      " 1.75535277e-01 1.22196531e+00 2.37673616e+00 0.00000000e+00\n",
      " 3.47471923e-01 5.95821381e-01 0.00000000e+00 2.21814656e+00\n",
      " 0.00000000e+00 3.23894453e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.55467474e+00 6.89323068e-01 4.57679093e-01\n",
      " 6.58413023e-02 2.63105512e+00 0.00000000e+00 2.94547844e+00\n",
      " 2.63701630e+00 1.22121286e+00 1.17666996e+00 2.41887763e-01\n",
      " 1.33084702e+00 0.00000000e+00 8.32516849e-01 9.97948721e-02\n",
      " 5.82986295e-01 1.16637513e-01 3.08871579e+00 1.66274941e+00\n",
      " 1.85316324e+00 1.16291177e+00 0.00000000e+00 0.00000000e+00\n",
      " 7.29347765e-01 0.00000000e+00 1.68581450e+00 2.53018975e+00\n",
      " 2.32290721e+00 0.00000000e+00 0.00000000e+00 2.58537918e-01\n",
      " 1.41884124e+00 3.11077386e-02 1.48235226e+00 2.67894673e+00\n",
      " 2.91401696e+00 0.00000000e+00 8.70868862e-01 6.38671160e-01\n",
      " 3.20799276e-03 2.91014016e-02 1.92305946e+00 0.00000000e+00\n",
      " 3.72218102e-01 1.08373857e+00 2.80224967e+00 2.68813205e+00\n",
      " 2.58698535e+00 0.00000000e+00 7.99516797e-01 0.00000000e+00\n",
      " 0.00000000e+00 1.07276607e+00 0.00000000e+00 6.32238805e-01\n",
      " 5.80374599e-01 0.00000000e+00 0.00000000e+00 8.59362245e-01\n",
      " 2.57967561e-01 0.00000000e+00 4.80463654e-02 8.53520393e-01\n",
      " 1.01427585e-01 3.08867908e+00 3.16052246e+00 2.97872925e+00]\n",
      "[2.2522154  1.0008266  0.         0.         0.         2.1823657\n",
      " 0.38254195 1.4317825  0.26266974 0.         0.08145374 1.2648665\n",
      " 0.         0.4102524  1.0812302  2.2072513  0.1196852  1.0362885\n",
      " 0.59168285 0.2651748  0.00916008 0.         1.5207527  2.140218\n",
      " 0.         2.8138397  2.1140335  0.95163876 0.         1.3521097\n",
      " 1.5204222  0.47086433 0.0223293  0.16301866 0.9046018  0.93214196\n",
      " 2.240818   1.5525069  0.21582082 0.36998907 0.05940687 0.9329503\n",
      " 0.5914507  0.11314465 0.         0.75055873 0.16635823 2.0492983\n",
      " 2.553199   0.803028   0.00980744 1.4064277  1.5818187  0.65301776\n",
      " 1.9670902  0.6468947  2.34253    1.4032928  0.         0.\n",
      " 0.19525522 1.2031043  0.61688316 0.         3.260931   0.15370092\n",
      " 1.0656232  0.2737087  0.8642649  1.77885    0.         0.01307339\n",
      " 0.         1.2953714  1.6541271  0.54279333 1.7993456  0.11975043\n",
      " 0.         0.03881451 0.         1.5761408  1.5028874  3.1938605\n",
      " 0.         0.56464636 0.8327076  1.97626    0.         0.4075771\n",
      " 0.47595865 0.         0.02361292 0.         1.4780266  0.33920538\n",
      " 1.6810813  2.0757525  1.1435719  0.43739656 2.1714888  0.\n",
      " 0.         2.889158   0.         1.1667303  0.5157943  2.2917361\n",
      " 0.         1.1969696  0.08475545 0.21604653 0.08022407 0.06833898\n",
      " 3.2611053  2.1254508  1.2086747  0.86465526 0.         0.0620447\n",
      " 0.21933153 2.755376   0.58871865 0.         1.5012486  0.6088751\n",
      " 0.0310099  0.03861466 0.43573153 0.         0.00679788 2.596817\n",
      " 0.         0.17028806 0.16534628 1.6222137  0.08222243 1.4776742\n",
      " 0.         1.446506   1.1958781  0.8697384  0.18786718 0.01341927\n",
      " 0.         3.1886892  0.4552495  2.3780339  0.21264337 1.4104438\n",
      " 2.0241795  0.         1.976558   0.         0.24073584 2.888373\n",
      " 0.         0.0145315  2.5487366  1.6532891  0.29993856 0.\n",
      " 0.         0.13668385 0.05935632 0.58576655 1.820923   0.01521148\n",
      " 0.21196802 1.945898   1.0648255  0.         0.36666077 0.\n",
      " 0.         0.01239207 1.3781967  0.         1.2383009  0.627105\n",
      " 0.20950614 1.5047295  0.         0.         0.         1.5160866\n",
      " 0.0400661  2.6643836  0.02157974 0.         0.         0.12489574\n",
      " 2.5404606  0.36678258 1.2334006  0.         0.17280394 0.0036476\n",
      " 0.03189059 0.         2.0238774  0.23737392 0.25455827 0.\n",
      " 2.3031502  0.1413316  0.1723822  0.03822735 0.02746457 2.342557\n",
      " 0.7398857  0.         1.9113097  2.8773193  1.9510068  0.54143023\n",
      " 0.53885007 0.20365117 0.         0.98104894 1.3759621  0.6129168\n",
      " 0.01272739 0.0472145  0.14890712 2.1008143  0.7493513  2.3975413\n",
      " 0.02915415 0.         1.5055444  0.06391567 1.2196442  0.4580819\n",
      " 0.         0.2827649  0.         0.         1.0749654  0.\n",
      " 0.33158317 1.9373859  0.         0.5127934  1.936531   0.7818887\n",
      " 0.         1.9183147  0.         2.1968699  0.20917557 1.0120679\n",
      " 0.14095084 0.3291392  0.63596094 0.3075098  2.248051   2.5838625\n",
      " 2.285029   0.         0.         0.         0.3903049  1.2068768\n",
      " 0.5766493  0.21482149 0.         0.24028298 1.4126133  0.45417422\n",
      " 0.         1.0177808  0.         0.         0.18074866 1.7260889\n",
      " 0.12800051 0.2605165  1.1710411  0.         1.1937274  0.3568919\n",
      " 0.         1.235798   0.01042562 1.4348519  0.15948674 0.\n",
      " 0.         2.4349728  2.4634812  0.02715003 0.3135595  0.07339165\n",
      " 0.07112025 0.06732687 1.1071965  1.841102   0.18998902 0.\n",
      " 0.         1.5808626  2.7170415  0.         1.3578622  0.966006\n",
      " 0.0276514  1.3721683  1.3625966  1.175382   0.62152666 0.9508819\n",
      " 0.02543949 0.20366104 0.         0.9252479  2.0550716  0.\n",
      " 0.05232283 0.21375856 0.1762866  0.         1.307145   0.66109353\n",
      " 0.03182229 0.         0.0447537  0.3480054  0.3353703  0.83323264\n",
      " 1.1616343  0.50693405 1.7320819  0.2770418  0.84535635 1.235678\n",
      " 0.13800947 0.12159248 0.         0.19176976 0.21497329 1.2048932\n",
      " 0.         0.8645957  0.931332   0.         2.3605742  0.\n",
      " 2.4408865  0.         2.8415346  1.1781502  0.         0.03990449\n",
      " 1.2979717  0.         1.8116665  1.1379025  1.3307554  0.6990696\n",
      " 0.06767353 0.42727837 0.37874857 0.27278006 0.48639795 0.\n",
      " 0.62493134 0.         0.         2.0084224  0.         0.03884189\n",
      " 0.34896916 2.9409802  1.116704   0.69708765 0.         0.3658986\n",
      " 0.1609162  2.2605727  0.35286126 0.24378903 2.0811427  0.\n",
      " 2.7453344  1.157955   0.9906987  2.6400764  2.6767745  0.16934459\n",
      " 0.5226208  1.8254045  0.         0.         0.75831604 0.\n",
      " 1.9432667  0.0163364  0.0558753  0.4156952  0.5424002  0.12832803\n",
      " 0.07211474 0.7653744  2.0148416  0.1926773  2.063225   0.82039493\n",
      " 0.3799488  0.44711462 0.05085371 0.         1.6023111  0.22957116\n",
      " 0.1916314  0.92720264 0.34604663 0.         0.         2.0029078\n",
      " 1.6851904  1.0992584  0.         0.693894   2.8376887  0.3637343\n",
      " 2.6943867  0.8603276  0.19666865 0.01391241 0.68526787 1.4686769\n",
      " 0.2248139  1.1866137  2.3357134  0.         0.30901533 0.57008594\n",
      " 0.         2.1980443  0.         3.2226257  0.         0.\n",
      " 0.         1.591092   0.74273235 0.47967857 0.08264697 2.6618183\n",
      " 0.         2.91474    2.5965362  1.1997504  1.151928   0.24905424\n",
      " 1.3276865  0.         0.7699501  0.09942508 0.5390268  0.1198004\n",
      " 3.12024    1.6350304  1.8010936  1.1169776  0.         0.\n",
      " 0.7684295  0.         1.7653044  2.5693827  2.3076751  0.\n",
      " 0.         0.24791428 1.3917881  0.07199449 1.4315622  2.6897013\n",
      " 2.9181182  0.         0.860156   0.6038528  0.         0.02432566\n",
      " 1.8884252  0.         0.36763698 1.0640094  2.8308961  2.7188063\n",
      " 2.5778418  0.         0.7639148  0.         0.         1.0419677\n",
      " 0.00948944 0.64923733 0.63543564 0.         0.         0.83965504\n",
      " 0.27618602 0.         0.04490916 0.8491657  0.10050367 3.0627413\n",
      " 3.128349   2.959661  ]\n"
     ]
    }
   ],
   "source": [
    "feat = ShufflePatchFeatureExtractor(\"/Users/racoon/Desktop/rotation_jigsaw_0490_0.0001_2.0414_41.84.pt\")\n",
    "\n",
    "patch_dim = 32\n",
    "\n",
    "image_paths = glob('/Users/racoon/Downloads/open-images-sample/*.jpg')\n",
    "image_index = int(math.floor((len(image_paths) * random.random())))\n",
    "\n",
    "cv2_image = cv2.imread(image_paths[image_index])\n",
    "\n",
    "pil_image = Image.open(image_paths[image_index]).convert('RGB')\n",
    "pil_image = np.array(pil_image)\n",
    "\n",
    "\n",
    "y_coord = int(math.floor((pil_image.shape[0] - patch_dim) * random.random()))\n",
    "x_coord = int(math.floor((pil_image.shape[1] - patch_dim) * random.random()))\n",
    "\n",
    "\n",
    "cv2_patch = cv2_image[y_coord:y_coord+patch_dim, x_coord:x_coord+patch_dim]\n",
    "cv2_patchs = np.expand_dims(cv2_patch, axis=0)\n",
    "\n",
    "pil_patch = pil_image[y_coord:y_coord+patch_dim, x_coord:x_coord+patch_dim]\n",
    "pil_patchs = np.expand_dims(pil_patch, axis=0)\n",
    "\n",
    "\n",
    "f1 = feat.evalRGB(pil_patchs)\n",
    "f2 = feat.evalBGR(cv2_patchs)\n",
    "\n",
    "from scipy.spatial import distance\n",
    "print( distance.cosine(f1.reshape((512,)), f2.reshape((512,))))\n",
    "\n",
    "print(f1.reshape((512,)) == f2.reshape((512,)))\n",
    "print(f1.reshape((512,)))\n",
    "print(f2.reshape((512,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint... /Users/racoon/Desktop/rotation_jigsaw_0490_0.0001_2.0414_41.84.pt\n",
      "odict_keys(['cnn.0.weight', 'cnn.0.bias', 'cnn.1.weight', 'cnn.1.bias', 'cnn.1.running_mean', 'cnn.1.running_var', 'cnn.1.num_batches_tracked', 'cnn.3.weight', 'cnn.3.bias', 'cnn.4.weight', 'cnn.4.bias', 'cnn.4.running_mean', 'cnn.4.running_var', 'cnn.4.num_batches_tracked', 'cnn.7.weight', 'cnn.7.bias', 'cnn.8.weight', 'cnn.8.bias', 'cnn.8.running_mean', 'cnn.8.running_var', 'cnn.8.num_batches_tracked', 'cnn.10.weight', 'cnn.10.bias', 'cnn.11.weight', 'cnn.11.bias', 'cnn.11.running_mean', 'cnn.11.running_var', 'cnn.11.num_batches_tracked', 'cnn.14.weight', 'cnn.14.bias', 'cnn.15.weight', 'cnn.15.bias', 'cnn.15.running_mean', 'cnn.15.running_var', 'cnn.15.num_batches_tracked', 'cnn.17.weight', 'cnn.17.bias', 'cnn.18.weight', 'cnn.18.bias', 'cnn.18.running_mean', 'cnn.18.running_var', 'cnn.18.num_batches_tracked', 'cnn.20.weight', 'cnn.20.bias', 'cnn.21.weight', 'cnn.21.bias', 'cnn.21.running_mean', 'cnn.21.running_var', 'cnn.21.num_batches_tracked', 'cnn.24.weight', 'cnn.24.bias', 'cnn.25.weight', 'cnn.25.bias', 'cnn.25.running_mean', 'cnn.25.running_var', 'cnn.25.num_batches_tracked', 'cnn.27.weight', 'cnn.27.bias', 'cnn.28.weight', 'cnn.28.bias', 'cnn.28.running_mean', 'cnn.28.running_var', 'cnn.28.num_batches_tracked', 'cnn.30.weight', 'cnn.30.bias', 'cnn.31.weight', 'cnn.31.bias', 'cnn.31.running_mean', 'cnn.31.running_var', 'cnn.31.num_batches_tracked', 'cnn.34.weight', 'cnn.34.bias', 'cnn.35.weight', 'cnn.35.bias', 'cnn.35.running_mean', 'cnn.35.running_var', 'cnn.35.num_batches_tracked', 'cnn.37.weight', 'cnn.37.bias', 'cnn.38.weight', 'cnn.38.bias', 'cnn.38.running_mean', 'cnn.38.running_var', 'cnn.38.num_batches_tracked', 'cnn.40.weight', 'cnn.40.bias', 'cnn.41.weight', 'cnn.41.bias', 'cnn.41.running_mean', 'cnn.41.running_var', 'cnn.41.num_batches_tracked', 'fc6.0.weight', 'fc6.0.bias', 'fc.0.weight', 'fc.0.bias', 'fc.3.weight', 'fc.3.bias'])\n",
      "cnn.0.weight\n",
      "cnn.0.bias\n",
      "cnn.1.weight\n",
      "cnn.1.bias\n",
      "cnn.1.running_mean\n",
      "cnn.1.running_var\n",
      "cnn.1.num_batches_tracked\n",
      "cnn.3.weight\n",
      "cnn.3.bias\n",
      "cnn.4.weight\n",
      "cnn.4.bias\n",
      "cnn.4.running_mean\n",
      "cnn.4.running_var\n",
      "cnn.4.num_batches_tracked\n",
      "cnn.7.weight\n",
      "cnn.7.bias\n",
      "cnn.8.weight\n",
      "cnn.8.bias\n",
      "cnn.8.running_mean\n",
      "cnn.8.running_var\n",
      "cnn.8.num_batches_tracked\n",
      "cnn.10.weight\n",
      "cnn.10.bias\n",
      "cnn.11.weight\n",
      "cnn.11.bias\n",
      "cnn.11.running_mean\n",
      "cnn.11.running_var\n",
      "cnn.11.num_batches_tracked\n",
      "cnn.14.weight\n",
      "cnn.14.bias\n",
      "cnn.15.weight\n",
      "cnn.15.bias\n",
      "cnn.15.running_mean\n",
      "cnn.15.running_var\n",
      "cnn.15.num_batches_tracked\n",
      "cnn.17.weight\n",
      "cnn.17.bias\n",
      "cnn.18.weight\n",
      "cnn.18.bias\n",
      "cnn.18.running_mean\n",
      "cnn.18.running_var\n",
      "cnn.18.num_batches_tracked\n",
      "cnn.20.weight\n",
      "cnn.20.bias\n",
      "cnn.21.weight\n",
      "cnn.21.bias\n",
      "cnn.21.running_mean\n",
      "cnn.21.running_var\n",
      "cnn.21.num_batches_tracked\n",
      "cnn.24.weight\n",
      "cnn.24.bias\n",
      "cnn.25.weight\n",
      "cnn.25.bias\n",
      "cnn.25.running_mean\n",
      "cnn.25.running_var\n",
      "cnn.25.num_batches_tracked\n",
      "cnn.27.weight\n",
      "cnn.27.bias\n",
      "cnn.28.weight\n",
      "cnn.28.bias\n",
      "cnn.28.running_mean\n",
      "cnn.28.running_var\n",
      "cnn.28.num_batches_tracked\n",
      "cnn.30.weight\n",
      "cnn.30.bias\n",
      "cnn.31.weight\n",
      "cnn.31.bias\n",
      "cnn.31.running_mean\n",
      "cnn.31.running_var\n",
      "cnn.31.num_batches_tracked\n",
      "cnn.34.weight\n",
      "cnn.34.bias\n",
      "cnn.35.weight\n",
      "cnn.35.bias\n",
      "cnn.35.running_mean\n",
      "cnn.35.running_var\n",
      "cnn.35.num_batches_tracked\n",
      "cnn.37.weight\n",
      "cnn.37.bias\n",
      "cnn.38.weight\n",
      "cnn.38.bias\n",
      "cnn.38.running_mean\n",
      "cnn.38.running_var\n",
      "cnn.38.num_batches_tracked\n",
      "cnn.40.weight\n",
      "cnn.40.bias\n",
      "cnn.41.weight\n",
      "cnn.41.bias\n",
      "cnn.41.running_mean\n",
      "cnn.41.running_var\n",
      "cnn.41.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "# Migrate Model\n",
    "\n",
    "model = ShufflePatchEvalNet().to(device)\n",
    "\n",
    "model_save_path = \"/Users/racoon/Desktop/rotation_jigsaw_0490_0.0001_2.0414_41.84.pt\"\n",
    "\n",
    "print('Loading Checkpoint...', model_save_path)\n",
    "\n",
    "checkpoint = torch.load(model_save_path, map_location=torch.device(device))\n",
    "\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "print(state_dict.keys())\n",
    "\n",
    "with torch.no_grad():\n",
    "    for k in state_dict.keys():\n",
    "        if k.startswith('cnn.'):\n",
    "            print(k)\n",
    "            rgetattr(model, k).copy_(state_dict[k])\n",
    "\n",
    "model_save_path = \"/Users/racoon/Desktop/rotation_jigsaw_migrated_0490_0.0001_2.0414_41.84.pt\"\n",
    "\n",
    "torch.save(\n",
    "{\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint... /Users/racoon/Desktop/variation_2b_migrated_0135_0.001_1.4328_63.80.pt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d7f1bf40eed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading Checkpoint...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Migrated Model\n",
    "model_save_path = \"/Users/racoon/Desktop/variation_2b_migrated_0135_0.001_1.4328_63.80.pt\"\n",
    "print('Loading Checkpoint...', model_save_path)\n",
    "checkpoint = torch.load(model_save_path, map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_dim = 32\n",
    "validation_image_paths = glob('/Users/racoon/Downloads/open-images-sample/*.jpg')\n",
    "reuse_image_count = 0\n",
    "\n",
    "class ShufflePatchDataset(Dataset):\n",
    "\n",
    "  def __init__(self, image_paths, patch_dim, length, transform=None):\n",
    "    self.image_paths = image_paths\n",
    "    self.patch_dim = patch_dim\n",
    "    self.length = length\n",
    "    self.transform = transform\n",
    "    self.image_reused = 0\n",
    "    self.min_width = self.patch_dim\n",
    "    \n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    # [y, x, chan], dtype=uint8, top_left is (0,0)\n",
    "    \n",
    "    image_index = int(math.floor((len(self.image_paths) * random.random())))\n",
    "    \n",
    "    if self.image_reused == 0:\n",
    "      pil_image = Image.open(self.image_paths[image_index]).convert('RGB')\n",
    "      self.pil_image = pil_image.resize((int(round(pil_image.size[0]/3)), int(round(pil_image.size[1]/3))))\n",
    "      self.image_reused = reuse_image_count\n",
    "    else:\n",
    "      self.image_reused -= 1\n",
    "\n",
    "    image = np.array(self.pil_image)\n",
    "\n",
    "    # If image is too small, try another image\n",
    "    if (image.shape[0] - self.min_width) <= 0 or (image.shape[1] - self.min_width) <= 0:\n",
    "        return self.__getitem__(index)\n",
    "    \n",
    "    y_coord = int(math.floor((image.shape[0] - self.patch_dim) * random.random()))\n",
    "    x_coord = int(math.floor((image.shape[1] - self.patch_dim) * random.random()))\n",
    "\n",
    "    patch = image[y_coord:y_coord+self.patch_dim, x_coord:x_coord+self.patch_dim]\n",
    "\n",
    "    if self.transform:\n",
    "      patch = self.transform(patch)\n",
    "\n",
    "    return patch\n",
    "\n",
    "\n",
    "valdataset = ShufflePatchDataset(validation_image_paths, patch_dim, 1,\n",
    "                         transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valdataset,\n",
    "                                        batch_size=1,\n",
    "                                        num_workers=0,\n",
    "                                        shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f5ea4d85f775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimg0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "feat.model.eval()\n",
    "\n",
    "preprocess_input =  transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "data = next(iter(valloader))\n",
    "\n",
    "\n",
    "img0 = data.to(device)\n",
    "output = feat.model(img0)\n",
    "output = output.cpu().detach().numpy()\n",
    "print(output.shape)\n",
    "output = output.reshape((512,))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
