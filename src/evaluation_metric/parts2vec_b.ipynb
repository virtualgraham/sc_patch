{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import ipyplot\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import compress\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import ipyplot\n",
    "import gensim\n",
    "from ast import literal_eval\n",
    "\n",
    "# from ShufflePatchModel16 import ShufflePatchFeatureExtractor\n",
    "# from VggFeatureExtractor import VggFeatureExtractor\n",
    "from MoCoFeatureExtractor import MoCoFeatureExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = glob(\"dataset_1000/train/*/*.jpg\")\n",
    "data_frame = pd.DataFrame({'train':train})\n",
    "data_frame.to_csv('dataset_1000_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and Utility methods for extracting patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 96\n",
    "stride = 72\n",
    "kp_margin = 16 # keypoint detector has a margin around image where it can not find keypoints\n",
    "n_clusters = 1000\n",
    "\n",
    "walk_length = 10\n",
    "walks_per_image = 100\n",
    "word_format_string = '{:03d}'\n",
    "\n",
    "cluster_patches_per_image = 20\n",
    "\n",
    "image_scales = [1]\n",
    "\n",
    "feature_dim = 2048\n",
    "cluster_file = f'clusters_{window_size}_{stride}_{n_clusters}.pkl'\n",
    "image_cluster_frid_file = f'image_cluster_grids_{window_size}_{stride}_{n_clusters}.npy'\n",
    "\n",
    "cnn = MoCoFeatureExtractor(params_path='/home/ubuntu/moco_v2_800ep_pretrain.pth.tar')\n",
    "\n",
    "image_files = glob(\"/home/ubuntu/dataset_1000/train/*/*.jpg\")[:10]\n",
    "\n",
    "\n",
    "def extract_windows(frame, pos, window_size):\n",
    "    windows = np.empty((len(pos), window_size, window_size, 3), dtype=np.uint8)\n",
    "\n",
    "    for i in range(len(pos)):\n",
    "        windows[i] = extract_window(frame, pos[i], window_size)\n",
    "\n",
    "    return windows\n",
    "\n",
    "\n",
    "def extract_window(frame, pos, window_size):\n",
    "    half_w = window_size/2.0\n",
    "\n",
    "    top_left = [int(round(pos[0]-half_w)), int(round(pos[1]-half_w))]\n",
    "    bottom_right = [top_left[0]+window_size, top_left[1]+window_size]\n",
    "\n",
    "    return frame[top_left[0]:bottom_right[0], top_left[1]:bottom_right[1]]\n",
    "\n",
    "\n",
    "\n",
    "def get_rad_grid(grid_pos, rad, grid_shape):\n",
    "\n",
    "    top_left = (grid_pos[0]-rad, grid_pos[1]-rad)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for i in range(2*rad+1):\n",
    "        p = (top_left[0]+i, top_left[1])\n",
    "        if p[0] >= 0 and p[1] >= 0 and p[0] < grid_shape[0] and p[1] < grid_shape[1]:\n",
    "            res.append(p)\n",
    " \n",
    "    for i in range(2*rad+1):\n",
    "        p = (top_left[0]+i, top_left[1]+(2*rad))\n",
    "        if p[0] >= 0 and p[1] >= 0 and p[0] < grid_shape[0] and p[1] < grid_shape[1]:\n",
    "            res.append(p)\n",
    "\n",
    "    for i in range(2*rad-1):\n",
    "        p = (top_left[0], top_left[1]+(i+1))\n",
    "        if p[0] >= 0 and p[1] >= 0 and p[0] < grid_shape[0] and p[1] < grid_shape[1]:\n",
    "            res.append(p)\n",
    "\n",
    "    for i in range(2*rad-1):\n",
    "        p = (top_left[0]+(2*rad), top_left[1]+(i+1))\n",
    "        if p[0] >= 0 and p[1] >= 0 and p[0] < grid_shape[0] and p[1] < grid_shape[1]:\n",
    "            res.append(p)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def next_pos(salient_grid_positions, grid_shape, current_position):\n",
    "    \n",
    "    if current_position is not None:\n",
    "\n",
    "        rad_grid = get_rad_grid(current_position, 1, grid_shape)\n",
    "\n",
    "        # print('rad_grid', current_position, rad_grid)\n",
    "        \n",
    "        if len(rad_grid) == 0:\n",
    "            print(\"frame empty?\")\n",
    "            \n",
    "        else:\n",
    "            random.shuffle(rad_grid)\n",
    "            for loc in rad_grid:\n",
    "                if loc in salient_grid_positions:\n",
    "                    return loc\n",
    "    \n",
    "    return random.sample(salient_grid_positions,1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample and cluster patches\n",
    "This simply uses a fixed grid system. Future patch sampling methods could incorporate an intrest point detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for image_scale in image_scales:\n",
    "    for idx, image_file in tqdm(enumerate(image_files), total=len(image_files)):\n",
    "        \n",
    "        pil_image = Image.open(image_file).convert('RGB')\n",
    "        pil_image = pil_image.resize((int(round(pil_image.size[0] * image_scale)), int(round(pil_image.size[1] * image_scale))))\n",
    "        image = np.array(pil_image)\n",
    "\n",
    "        if image.shape[0] < window_size * 2 or image.shape[1] < window_size * 2:\n",
    "            continue\n",
    "            \n",
    "        margin = max(window_size, kp_margin*2)\n",
    "        grid_shape = (math.floor((image.shape[0] - margin) / stride), math.floor((image.shape[1] - margin) / stride))\n",
    "        offsets = (round((image.shape[0] - grid_shape[0] * stride)/2), round((image.shape[1] - grid_shape[1] * stride)/2))\n",
    "\n",
    "        points = [(offsets[0]+y*stride+stride/2,offsets[1]+x*stride+stride/2) for y in range(grid_shape[0]) for x in range(grid_shape[1])]\n",
    "        \n",
    "        if len(points) > cluster_patches_per_image:\n",
    "            points = random.sample(points, cluster_patches_per_image)\n",
    "        \n",
    "        patches = extract_windows(image, points, window_size)\n",
    "\n",
    "        windows = patches.astype(np.float64)\n",
    "\n",
    "        try:\n",
    "            feats = cnn.evalRGB(windows)\n",
    "        except:\n",
    "            print(\"ERROR windows.shape\", windows.shape)\n",
    "            raise\n",
    "            \n",
    "        feats = feats.reshape((windows.shape[0], feature_dim))\n",
    "        X.extend(list(feats))\n",
    "\n",
    "\n",
    "print(\"Clustering with KMeans: len(X)\", len(X))\n",
    "\n",
    "clusters = KMeans(n_clusters=n_clusters, verbose=False)\n",
    "clusters.fit(np.array(X, dtype=np.float32))\n",
    "\n",
    "pickle.dump(clusters, open(cluster_file, \"wb\"))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sequence dataset with random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_locations(grid_shape, stride, offsets, mask=None):\n",
    "    \n",
    "    if mask is not None:\n",
    "        object_grid_locations = set()\n",
    "\n",
    "        for y in range(grid_shape[0]):\n",
    "            for x in range(grid_shape[1]):\n",
    "                p = (offsets[0] + y * stride + 0.5 * stride, offsets[1] + x * stride + 0.5 * stride)\n",
    "                w = extract_window(mask, p, stride)\n",
    "\n",
    "                if np.sum(w) >= stride * stride * 0.3:\n",
    "                    object_grid_locations.add((y, x))\n",
    "        \n",
    "        return object_grid_locations\n",
    "    \n",
    "    else:\n",
    "        return [(y,x) for y in range(grid_shape[0]) for x in range(grid_shape[1])]\n",
    "    \n",
    "def generate_image_cluster_grid(image_file, image_scale, clusters, feature_extractor):\n",
    "    # print(\"generate_image_sequences\", image_file)\n",
    "\n",
    "    pil_image = Image.open(image_file).convert('RGB')\n",
    "    pil_image = pil_image.resize((int(round(pil_image.size[0] * image_scale)), int(round(pil_image.size[1] * image_scale))))\n",
    "    image = np.array(pil_image)\n",
    "\n",
    "    if image.shape[0] < window_size * 2 or image.shape[1] < window_size * 2:\n",
    "        print(\"image too small, image_file\")\n",
    "        return None\n",
    "            \n",
    "    margin = max(window_size, kp_margin*2)\n",
    "    grid_shape = (math.floor((image.shape[0] - margin) / stride), math.floor((image.shape[1] - margin) / stride))\n",
    "    offsets = (round((image.shape[0] - grid_shape[0] * stride)/2), round((image.shape[1] - grid_shape[1] * stride)/2))\n",
    "\n",
    "    grid_locations_set = get_grid_locations(grid_shape, stride, offsets)\n",
    "    grid_locations_list = list(grid_locations_set)\n",
    "    \n",
    "    points = [(y*stride + stride/2 + offsets[0], x*stride + stride/2 + offsets[1]) for (y,x) in grid_locations_list]\n",
    "        \n",
    "    patches = extract_windows(image, points, window_size)\n",
    "    windows = patches.astype(np.float64)\n",
    "\n",
    "    # print(windows.shape)\n",
    "    \n",
    "    feats = cnn.evalRGB(windows)\n",
    "    feats = feats.reshape((windows.shape[0], feature_dim))\n",
    "\n",
    "    grid_cluster_ids = clusters.predict(feats)\n",
    "        \n",
    "    cluster_grid = np.full(grid_shape, -1, dtype=int)\n",
    "    \n",
    "    for i in range(len(grid_locations_list)):\n",
    "        cluster_grid[grid_locations_list[i]] = grid_cluster_ids[i]\n",
    "        \n",
    "    return cluster_grid\n",
    "\n",
    "def generate_image_sequences(image_file, image_scale, clusters, feature_extractor, mask_file = None, seq_count=walks_per_image):\n",
    "    # print(\"generate_image_sequences\", image_file)\n",
    "\n",
    "    pil_image = Image.open(image_file).convert('RGB')\n",
    "    pil_image = pil_image.resize((int(round(pil_image.size[0] * image_scale)), int(round(pil_image.size[1] * image_scale))))\n",
    "    image = np.array(pil_image)\n",
    "\n",
    "    if image.shape[0] < window_size * 2 or image.shape[1] < window_size * 2:\n",
    "        print(\"image too small, image_file\")\n",
    "        return None, None\n",
    "            \n",
    "    mask = None\n",
    "    \n",
    "    if mask_file is not None:\n",
    "        pil_mask = Image.open(mask_file).convert('1')\n",
    "        pil_mask = pil_mask.resize((int(round(pil_mask.size[0] * image_scale)), int(round(pil_mask.size[1] * image_scale))))\n",
    "        mask = np.array(pil_mask)\n",
    "        \n",
    "    \n",
    "    margin = max(window_size, kp_margin*2)\n",
    "    grid_shape = (math.floor((image.shape[0] - margin) / stride), math.floor((image.shape[1] - margin) / stride))\n",
    "    offsets = (round((image.shape[0] - grid_shape[0] * stride)/2), round((image.shape[1] - grid_shape[1] * stride)/2))\n",
    "\n",
    "    grid_locations_set = get_grid_locations(grid_shape, stride, offsets, mask)\n",
    "    grid_locations_list = list(grid_locations_set)\n",
    "    \n",
    "    points = [(y*stride + stride/2 + offsets[0], x*stride + stride/2 + offsets[1]) for (y,x) in grid_locations_list]\n",
    "        \n",
    "    patches = extract_windows(image, points, window_size)\n",
    "    windows = patches.astype(np.float64)\n",
    "\n",
    "    # print(windows.shape)\n",
    "    \n",
    "    feats = cnn.evalRGB(windows)\n",
    "    feats = feats.reshape((windows.shape[0], feature_dim))\n",
    "\n",
    "    grid_cluster_ids = clusters.predict(feats)\n",
    "\n",
    "    grid_location_to_cluster_id = dict([(grid_locations_list[i], grid_cluster_ids[i]) for i in range(len(grid_locations_list))])\n",
    "        \n",
    "    cluster_seqs = []\n",
    "    point_seqs = []\n",
    "    \n",
    "    for i in range(seq_count):\n",
    "        cluster_seq = []\n",
    "        point_seq = []\n",
    "        \n",
    "        pos = None\n",
    "        \n",
    "        for t in range(walk_length):\n",
    "            pos = next_pos(grid_locations_set, grid_shape, pos)\n",
    "            cluster_seq.append(grid_location_to_cluster_id[pos])\n",
    "            point_seq.append((pos[0]*stride + stride/2 + offsets[0], pos[1]*stride + stride/2 + offsets[1]))\n",
    "            \n",
    "        cluster_seqs.append([word_format_string.format(w) for w in cluster_seq])\n",
    "        point_seqs.append(point_seq)\n",
    "        \n",
    "    return cluster_seqs, point_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "{'763e6730eeb67b95': array([[286, 837, 837, 837, 202, 687, 687, 687, 837, 286,  91,  91],\n",
      "       [ 91, 837, 799, 799, 202, 202, 286, 202, 202,  91, 837, 837],\n",
      "       [ 91, 677,  30,  88,  31, 837,  91, 286, 202, 687, 687, 837],\n",
      "       [ 91, 837, 401, 803, 996, 965, 286, 837, 599,  18, 266,  91],\n",
      "       [ 91, 837, 885, 885, 965, 233, 523, 632, 599, 202, 806, 806],\n",
      "       [806,  31, 885, 965, 601,  42, 632, 622,  91, 202, 286, 806],\n",
      "       [806,  61,  61, 632,   8,   8, 601, 286,  91, 806, 286, 286],\n",
      "       [286, 837, 275, 879, 171, 601,  15, 286, 806, 806, 286, 806]]), '78f43b2cda7e7083': array([[171, 599, 940, 940, 940, 940, 837, 286, 837, 837, 837, 286],\n",
      "       [599, 709, 171, 940, 171, 171, 940, 171, 233, 996, 996, 837],\n",
      "       [503, 233, 599, 599, 599, 940, 171, 171, 502, 996, 996, 996],\n",
      "       [ 17, 263, 940, 601, 996, 266, 233, 996, 996, 242, 996, 996],\n",
      "       [474, 996, 824, 601, 996, 996, 996, 996, 996, 996, 996, 996],\n",
      "       [ 91, 286, 502, 774, 774, 774, 996, 433, 242, 996, 996, 774],\n",
      "       [ 91,  91, 806, 222, 774, 774, 474, 474, 474, 774, 503, 842],\n",
      "       [ 91,  91,  91, 837, 284, 433, 209, 474, 284, 284,  47, 515]]), 'fc0a86f6d386a2a8': array([[202, 202, 202, 970,  95,  95, 469, 469, 970],\n",
      "       [202, 761, 970, 970, 970, 970, 970, 970, 970],\n",
      "       [847, 866, 479, 970, 349, 260, 260, 242, 242],\n",
      "       [837, 687, 761, 149, 236, 458, 687, 806,  91],\n",
      "       [286, 837, 655, 799,  88, 711, 837,  91, 806],\n",
      "       [806, 286,  38, 394, 712, 687, 837,  91,  91],\n",
      "       [286, 697, 479, 362, 712, 687, 687, 837, 687],\n",
      "       [854, 697, 655, 362, 712, 599, 632, 632, 523],\n",
      "       [202, 286, 202, 622, 775, 689,   8,  31, 806],\n",
      "       [854, 202, 655, 893, 632, 687, 286, 286, 202],\n",
      "       [854, 761, 202, 479, 400, 202, 593, 202, 854],\n",
      "       [ 91, 286, 202, 202, 202, 655, 202, 286, 655]]), 'ef83690bc161cd76': array([[781, 785, 361, 533, 778, 430, 693, 921, 921, 837, 533, 340],\n",
      "       [926, 404, 533, 104, 357, 125,  40, 921, 921, 271, 533, 799],\n",
      "       [360, 361, 533, 451,  28, 684,  30, 921, 921, 294, 104, 149],\n",
      "       [702, 149, 568,  17, 430, 451,  35, 921, 785,  40, 758, 780],\n",
      "       [799, 260, 260, 355, 104, 560, 191, 785, 785, 632, 523, 780],\n",
      "       [687, 260, 149, 435,  28, 560, 439, 632, 689,  18, 222, 191],\n",
      "       [226, 260, 149, 655, 282,  40, 363, 893, 775, 965, 222, 562],\n",
      "       [149, 430,  67, 655,   2, 149,  18, 893, 785, 687, 191, 502],\n",
      "       [191, 430, 149, 149, 361, 479, 599,  18, 599, 655, 191, 149]]), '001d7ce89aad2f7a': array([[ 47, 242, 450, 806, 785, 326, 431, 720, 806, 806, 785, 806],\n",
      "       [672, 629, 242, 921, 785, 846, 224, 687, 538, 538, 785, 806],\n",
      "       [629,  72, 282, 502, 401, 846,  30, 687, 242, 806, 785, 806],\n",
      "       [  2,   2,  31, 213,  40,  40, 389, 837, 785, 785, 785, 286],\n",
      "       [  2,   2, 260, 430, 965, 965,  61, 361, 785, 785, 687, 560],\n",
      "       [379,   2, 260, 785, 430, 599, 599,  18, 775, 689, 689, 854],\n",
      "       [ 72,   2, 806, 785, 430,  18,  42, 599, 622, 174, 785, 697],\n",
      "       [364, 916, 286, 806, 806, 286, 848, 560, 785, 785, 286, 286],\n",
      "       [ 17,   2,   2, 260, 286, 286, 560, 510, 601, 785, 785, 806]]), '79823c72bd710337': array([[ 91,  91,  91,  91,  91,  91,  91,  91,  91,  91],\n",
      "       [806, 720, 533,  91,  91,  91,  91,  91,  91,  91],\n",
      "       [837, 199,  81, 837,  91,  91,  91,  91,  91,  91],\n",
      "       [845,  81, 846, 321,  91,  91,  91,  91,  91,  91],\n",
      "       [806, 803, 321, 693, 806,  91,  91,  91,  91,  91],\n",
      "       [837, 351, 803, 668, 687,  91,  91,  91, 837, 687],\n",
      "       [837, 776, 965, 730, 608, 687, 806, 286,  18,  18],\n",
      "       [286, 965, 965, 776, 608, 401, 697, 599, 363, 965],\n",
      "       [ 91, 286, 965, 723,  61, 275, 275, 632, 879, 260],\n",
      "       [ 91,  91, 837, 351, 275, 781, 750, 122, 697, 286],\n",
      "       [ 91,  91, 286, 693,  15, 650, 599, 668, 693, 479],\n",
      "       [ 91, 806, 286, 450, 730, 364, 754, 642, 781, 164]]), '22dc051d032eb310': array([[684, 684,  65, 697, 242, 670, 670, 242, 697, 697, 697, 697],\n",
      "       [684, 684, 684, 538, 538, 502, 502, 785, 538, 697, 697, 697],\n",
      "       [684, 684, 684, 422, 684, 684, 684, 502, 806, 242, 242, 538],\n",
      "       [735, 735, 684, 439, 684, 684, 655, 785, 538, 804, 538, 538],\n",
      "       [622, 684,  67, 684, 684, 684,  17, 502, 286, 242, 242, 806],\n",
      "       [622, 684, 684,  31, 430, 502, 430, 785, 286, 242, 806, 806],\n",
      "       [ 40, 684, 126, 785, 806, 785, 785, 785, 286, 806, 806, 806],\n",
      "       [430, 126, 430, 260, 260, 260, 655, 202, 202, 242, 286, 785],\n",
      "       [430, 943, 430, 579, 655, 149, 697, 260, 502, 286, 260, 785]]), '584b6b47f4da18be': array([[450,  76, 260, 533, 711, 533, 450,  72],\n",
      "       [450, 450, 149,  81, 321, 431, 379, 655],\n",
      "       [486, 761, 523, 778, 523, 362, 401, 273],\n",
      "       [364, 876, 286, 803, 893, 668,  61, 965],\n",
      "       [450,  26, 355, 622, 893,  63, 668, 603],\n",
      "       [ 72, 722, 722, 667, 668, 668, 668, 603],\n",
      "       [ 26,  72,  72, 604, 668, 242, 965, 603],\n",
      "       [450, 430, 123, 824, 824, 785, 202, 282],\n",
      "       [296, 474, 622, 360, 824, 806, 806, 965],\n",
      "       [565, 479, 523, 824, 176, 785, 806, 965],\n",
      "       [565, 479, 874, 885, 645, 176, 176, 176],\n",
      "       [479, 479,  38, 132, 176, 176, 645,  80]]), '26878ebeea804301': array([[  0, 520, 538, 538, 538, 538, 538, 538, 538, 538, 538, 538],\n",
      "       [469, 520, 538, 837, 622, 799, 837, 538, 687, 479, 538, 538],\n",
      "       [538, 538,   0, 799,  88,  30, 799,  42, 523, 479, 538, 806],\n",
      "       [538, 538, 388, 401, 401, 823, 775,   8, 242, 538, 538, 806],\n",
      "       [314, 701, 364, 132, 668, 775,   8, 965, 538, 538, 538, 806],\n",
      "       [314, 174, 132, 616, 616, 893, 893, 388, 538, 538, 538, 806],\n",
      "       [314, 174, 670, 202, 655, 668, 723, 538, 538, 538, 538, 286],\n",
      "       [314, 701, 701, 132, 655, 965, 723, 538, 538, 538, 538, 242]]), 'b22d9de882c2d468': array([[806, 806, 520, 520, 538,   0, 310, 799,  31, 523, 364, 806],\n",
      "       [242, 242, 538, 538, 974, 401, 401, 799, 735, 388, 364, 479],\n",
      "       [  0, 538, 538, 538, 929, 401, 286, 965, 668, 388, 594, 364],\n",
      "       [ 95, 538, 538, 388, 401, 965, 806, 965, 965, 668, 242, 974],\n",
      "       [697,  95, 538, 510, 510, 837, 286, 965, 965, 965, 314, 974],\n",
      "       [126,  95, 538, 388,  31, 221, 668, 668, 603,  83, 388, 974],\n",
      "       [469, 538, 538,  83,  28, 684, 246, 934, 599, 735, 560, 202],\n",
      "       [538, 325, 314, 543, 622, 560, 479, 599, 697, 599, 599,  31],\n",
      "       [ 95, 325, 388, 760, 479, 202, 654, 399, 697, 286, 286, 286]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clusters = pickle.load(open(cluster_file, \"rb\"))\n",
    "\n",
    "image_cluster_grids = {}\n",
    "\n",
    "for image_scale in image_scales:\n",
    "    for idx, image_file in tqdm(enumerate(image_files), total=len(image_files)):\n",
    "\n",
    "        image_cluster_grid = generate_image_cluster_grid(image_file, image_scale, clusters, cnn)\n",
    "        if image_cluster_grid is None:\n",
    "            continue\n",
    "        image_id = image_file.split('/')[-1].split('.')[0]\n",
    "        image_cluster_grids[image_id] = image_cluster_grid    \n",
    "    \n",
    "np.save(image_cluster_frid_file, image_cluster_grids)   \n",
    "\n",
    "print(\"done\")\n",
    "print(np.load(image_cluster_frid_file, allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 9245/15537 [24:48<16:52,  6.21it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c293266093bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mc_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_image_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc_seqs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-b3db68cc4cfb>\u001b[0m in \u001b[0;36mgenerate_image_sequences\u001b[0;34m(image_file, image_scale, clusters, feature_extractor, mask_file, seq_count)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# print(windows.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevalRGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sc_patch/src/evaluation_metric/MoCoFeatureExtractor.py\u001b[0m in \u001b[0;36mevalRGB\u001b[0;34m(self, patches)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sc_patch/src/evaluation_metric/MoCoFeatureExtractor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clusters = pickle.load(open(cluster_file, \"rb\"))\n",
    "\n",
    "cluster_seqs = []\n",
    "point_seqs = []\n",
    "image_file_colummn = []\n",
    "image_scale_colummn = []\n",
    "\n",
    "for image_scale in image_scales:\n",
    "    for idx, image_file in tqdm(enumerate(image_files), total=len(image_files)):\n",
    "\n",
    "        c_seqs, p_seqs = generate_image_sequences(image_file, image_scale, clusters, cnn)\n",
    "        if c_seqs is None:\n",
    "            continue\n",
    "        cluster_seqs.extend(c_seqs)\n",
    "        point_seqs.extend(p_seqs)\n",
    "        image_file_colummn.extend([image_file] * walks_per_image)\n",
    "        image_scale_colummn.extend([image_scale] * walks_per_image)\n",
    "    \n",
    "    \n",
    "data_frame = pd.DataFrame({'words':cluster_seqs, 'file':image_file_colummn, 'points': point_seqs, 'scale': image_scale_colummn})\n",
    "\n",
    "data_frame.to_csv('sequences.csv')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print('epoch {}'.format(self.epoch))\n",
    "        self.epoch += 1\n",
    "              \n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    data_frame = pd.read_csv('sequences.csv',converters={\"words\": literal_eval, \"points\": literal_eval})\n",
    "    \n",
    "    for index, row in data_frame.iterrows():\n",
    "        if tokens_only:\n",
    "            yield row['words']\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(row['words'], [index])\n",
    "\n",
    "train_corpus = list(read_corpus('sequences.csv'))\n",
    "print(train_corpus[:2])\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, epochs=40)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs, callbacks=[callback()])\n",
    "\n",
    "model.save('doc2vec.model')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pickle.load(open(\"clusters.pkl\", \"rb\"))\n",
    "orb = cv2.ORB_create(nfeatures=100000, fastThreshold=7)\n",
    "model = gensim.models.doc2vec.Doc2Vec.load('doc2vec.model')\n",
    "\n",
    "data_frame = pd.read_csv('sequences.csv',converters={\"words\": literal_eval, \"points\": literal_eval})\n",
    "\n",
    "test_image_files = glob(\"/data/dataset_100/test/*/*.jpg\")\n",
    "test_mask_files = glob(\"/data/dataset_100/test/*/*.mask.png\")\n",
    "\n",
    "test_image_files.sort()\n",
    "test_mask_files.sort()\n",
    "\n",
    "\n",
    "for image_scale in image_scales:\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(len(test_image_files)):\n",
    "\n",
    "        image_file = test_image_files[i]\n",
    "        mask_file = test_mask_files[i]\n",
    "\n",
    "        print(\"test\", image_file)\n",
    "\n",
    "        c_seqs, p_seqs = generate_image_sequences(image_file, image_scale, clusters, cnn, orb, mask_file = mask_file, seq_count=100)\n",
    "        #print('seqs', seqs)\n",
    "\n",
    "        vectors = [[model.infer_vector(s)] for s in c_seqs]\n",
    "        #print('vectors', vectors)\n",
    "\n",
    "        for v in vectors:\n",
    "            similar = model.docvecs.most_similar(v, topn=10)\n",
    "            #print('similar', similar)\n",
    "\n",
    "            for s in similar:\n",
    "                f = data_frame.loc[s[0],'file']\n",
    "                if '/airplane/' in image_file and '/airplane/' in f:\n",
    "                    correct += 1\n",
    "                elif '/car/' in image_file and '/car/' in f:\n",
    "                    correct += 1\n",
    "                elif '/horse/' in image_file and '/horse/' in f:\n",
    "                    correct += 1\n",
    "\n",
    "                #print(\"similar to\", f)\n",
    "\n",
    "                total += 1\n",
    "    \n",
    "    print(\"score\", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
